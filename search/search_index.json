{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Glasses \ud83d\ude0e A long way to go \ud83d\udea7 This project is WIP. We will make it perfect, but we are not still there! If you want to help out, check the Contributing Guide \ud83d\udc9c. A compact, concise, and customizable deep learning library. This library currently supports deep learning models for computer vision. Glasses is a model toolbox to make it easier for everybody to use, learn and share deep learning models. Documentation : TODO Source Code : https://github.com/FrancescoSaverioZuppichini/glasses TL;DR This library has human readable code, no research code common component are shared across models same APIs for all models (you learn them once and they are always the same) clear and easy to use model constomization (see here ) classification and segmentation easy to contribute, see the contribution guide emoji in the name ;) Requirements Python 3.8+ Installation $ pip install git+https://github.com/FrancescoSaverioZuppichini/glasses.git ---> 100% Motivations Almost all existing implementations of the most famous model are written with very bad coding practices, what today is called research code. We struggled to understand some of the implementations even if in the end were just a few lines of code. Most of them are missing a global structure, they used tons of code repetition, and they are not easily customizable and not tested. Thus, not easy to share and use by everybody. Head over the getting started guide RoadMap We plan to have three main steps in the development Models : Defined different models for different tasks, the configuration system, and how to save/load them. \u2b05\ufe0f We are here! Tasks : Defined the train/evaluation lifecycle for each task. Pipelines : Defined the whole lifecycle for a task, from data to training. Contributing Please contribute using GitHub Flow . Create a branch, add commits, and open a pull request. Please read contributing for details on our CODE OF CONDUCT, and the process for submitting pull requests to us. License\u00b6 This project is licensed under the terms of the MIT license.","title":"Glasses"},{"location":"#glasses","text":"A long way to go \ud83d\udea7 This project is WIP. We will make it perfect, but we are not still there! If you want to help out, check the Contributing Guide \ud83d\udc9c. A compact, concise, and customizable deep learning library. This library currently supports deep learning models for computer vision. Glasses is a model toolbox to make it easier for everybody to use, learn and share deep learning models. Documentation : TODO Source Code : https://github.com/FrancescoSaverioZuppichini/glasses","title":"Glasses \ud83d\ude0e"},{"location":"#tldr","text":"This library has human readable code, no research code common component are shared across models same APIs for all models (you learn them once and they are always the same) clear and easy to use model constomization (see here ) classification and segmentation easy to contribute, see the contribution guide emoji in the name ;)","title":"TL;DR"},{"location":"#requirements","text":"Python 3.8+","title":"Requirements"},{"location":"#installation","text":"$ pip install git+https://github.com/FrancescoSaverioZuppichini/glasses.git ---> 100%","title":"Installation"},{"location":"#motivations","text":"Almost all existing implementations of the most famous model are written with very bad coding practices, what today is called research code. We struggled to understand some of the implementations even if in the end were just a few lines of code. Most of them are missing a global structure, they used tons of code repetition, and they are not easily customizable and not tested. Thus, not easy to share and use by everybody. Head over the getting started guide","title":"Motivations"},{"location":"#roadmap","text":"We plan to have three main steps in the development Models : Defined different models for different tasks, the configuration system, and how to save/load them. \u2b05\ufe0f We are here! Tasks : Defined the train/evaluation lifecycle for each task. Pipelines : Defined the whole lifecycle for a task, from data to training.","title":"RoadMap"},{"location":"#contributing","text":"Please contribute using GitHub Flow . Create a branch, add commits, and open a pull request. Please read contributing for details on our CODE OF CONDUCT, and the process for submitting pull requests to us.","title":"Contributing"},{"location":"#license","text":"This project is licensed under the terms of the MIT license.","title":"License\u00b6"},{"location":"alternatives/","text":"What inspired Glasses , how it compares to other alternatives and what it learned from them. Intro Glasses wouldn't exist if not for the previous work of others and the f* amazing open-source community . We love every single one of you \ud83e\udd70. Transformers Transformers is not a model toolbox, therefore you cannot compose and share individual building components. Moreover, their philosophy of one model one file creates a lot of code repetition creating huge model files hard to read and understand. Their testing approach, even if more robust than ours, increase the development time due to the amount of work required to make the model pass the tests. Moreover, we are not motivated by financial gains. Thus, we don't follow the hype blindly. Inspired Glasses to How to call the AutoModel class I guess. IceVision IceVision is a great library to train models in different vision tasks. The main difference between glasses is that they don't \"own\" the model, they rely on third-party libraries with custom adapters. This strategy makes it easier to increase the pool of available models, but a minor change in one dependency may break the whole codebase. Our goal is also to teach , for this reason, we implemented all the models we use in a (hopefully) clear and concise way. Inspired Glasses to Provide a simple way to defined tasks' inputs/outsputs. OpenMMLab OpenMMLab team has different amazing libraries for each vision task. However, they are fragmented and not easy to use due to the configuration system. Their configuration system is not typed, therefore is impossible for the end-user to know what to place inside it. They used inheritance in configuration, making it challenging to have a full view of the system. Finally, their Trainer is a closed box; very hard to extend. We will rely on Lightning Inspired Glasses to Create nice and composable configurations. Detectron2 If you know how use it, you are my hero.","title":"Alternatives, Inspiration and Comparisons"},{"location":"alternatives/#intro","text":"Glasses wouldn't exist if not for the previous work of others and the f* amazing open-source community . We love every single one of you \ud83e\udd70.","title":"Intro"},{"location":"alternatives/#transformers","text":"Transformers is not a model toolbox, therefore you cannot compose and share individual building components. Moreover, their philosophy of one model one file creates a lot of code repetition creating huge model files hard to read and understand. Their testing approach, even if more robust than ours, increase the development time due to the amount of work required to make the model pass the tests. Moreover, we are not motivated by financial gains. Thus, we don't follow the hype blindly. Inspired Glasses to How to call the AutoModel class I guess.","title":"Transformers"},{"location":"alternatives/#icevision","text":"IceVision is a great library to train models in different vision tasks. The main difference between glasses is that they don't \"own\" the model, they rely on third-party libraries with custom adapters. This strategy makes it easier to increase the pool of available models, but a minor change in one dependency may break the whole codebase. Our goal is also to teach , for this reason, we implemented all the models we use in a (hopefully) clear and concise way. Inspired Glasses to Provide a simple way to defined tasks' inputs/outsputs.","title":"IceVision"},{"location":"alternatives/#openmmlab","text":"OpenMMLab team has different amazing libraries for each vision task. However, they are fragmented and not easy to use due to the configuration system. Their configuration system is not typed, therefore is impossible for the end-user to know what to place inside it. They used inheritance in configuration, making it challenging to have a full view of the system. Finally, their Trainer is a closed box; very hard to extend. We will rely on Lightning Inspired Glasses to Create nice and composable configurations.","title":"OpenMMLab"},{"location":"alternatives/#detectron2","text":"If you know how use it, you are my hero.","title":"Detectron2"},{"location":"contributing/","text":"Everyone is welcome to contribute \ud83d\udc9c Please contribute using GitHub Flow . Create a branch, add commits, and open a pull request. Be sure to check our coding style guide .","title":"Contributing"},{"location":"getting-started/","text":"$ AutoModel () . from_pretrained ( \"my_name\" ) [ 06 / 21 / 22 18 : 13 : 40 ] WARNING Error ( s ) in loading state_dict for AnyModelForImageClassification : base . py : 54 Unexpected key ( s ) in state_dict : \"head.fc.weight\" , \"head.fc.bias\" . INFO Loaded pretrained weights for dummy - d0 .","title":"Getting Started"},{"location":"reference/","text":"","title":"Index"},{"location":"reference/SUMMARY/","text":"glasses config logger models auto base model_zoo utils transform transform vision auto backbones base vit config model zoo image classification base common config model heads base linear_head config model vit config model outputs vit config zoo detection outputs segmentation outputs necks base nn storage base huggingface local types utils model_tester","title":"SUMMARY"},{"location":"reference/logger/","text":"","title":"logger"},{"location":"reference/types/","text":"","title":"types"},{"location":"reference/config/","text":"Config dataclass Base class for configurations, all configuration must inherit from this class. For a in depth tutorial about configs, head over Configurations Important Models are not coupled with Config , therefore they are unaware of the configuration system. Each Config is linked to a specific model, not viceversa. Note A Config holds what is needed to create a model. Therefore, they are perfect to share custom version of a specific architecture. A Config is data container . Thus, it must not have any side effect , any logic that requires the Config values to be somehow processed must be implemented in the Config.build function. A custom configuration can be written as follows: from glasses.config import Config # Assume we have a model class MyModel ( nn . Module ): def __init__ ( in_channels : int , out_channels : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) def forward ( self , x ): return self . conv ( x ) # Let's create it's configuration @dataclass class MyConfig ( Config ): in_channels : int out_channels : int def build ( self ) -> nn . Module : # create a `MyModel` instance using `MyConfig` return MyModel ( ** self . __dict__ ) model : MyModel = MyConfig ( 2 , 2 ) . build () Each Config is linked to a specific model, not viceversa. Models had no idea about the configuration system and can be created normally as you expect with their constructor. Nested Configurations Source code in glasses/config/__init__.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @dataclass class Config : \"\"\" Base class for configurations, all configuration **must** inherit from this class. For a in depth tutorial about configs, head over [Configurations](/) !!! important Models are not coupled with `Config`, therefore they are unaware of the configuration system. Each `Config` is linked to a specific model, not viceversa. !!! note A `Config` holds what is needed to create a model. Therefore, they are perfect to share custom version of a specific architecture. A `Config` is **data container**. Thus, it **must not have any side effect**, any logic that requires the `Config` values to be somehow processed must be implemented in the [`Config.build`](#glasses.config.Config.build) function. A custom configuration can be written as follows: ```python from glasses.config import Config # Assume we have a model class MyModel(nn.Module): def __init__(in_channels: int, out_channels: int): super().__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) def forward(self, x): return self.conv(x) # Let's create it's configuration @dataclass class MyConfig(Config): in_channels: int out_channels: int def build(self) -> nn.Module: # create a `MyModel` instance using `MyConfig` return MyModel(**self.__dict__) model: MyModel = MyConfig(2, 2).build() ``` Each Config is linked to a specific model, not viceversa. Models had no idea about the configuration system and can be created normally as you expect with their constructor. ## Nested Configurations \"\"\" def build ( self ) -> nn . Module : raise NotImplementedError def panel ( self ) -> Panel : table = Table ( title = self . __class__ . __name__ , box = box . SIMPLE , show_header = False , highlight = True , ) for key , val in self . __dict__ . items (): # if one field is a Config instance if isinstance ( val , Config ): table . add_row ( \"\" , \"\" ) table . add_row ( f \"[bold] { key } \" , val . panel ()) else : table . add_row ( key , str ( val )) return Panel ( table ) def summary ( self ): console = Console () console . print ( self . panel (), justify = \"left\" )","title":"config"},{"location":"reference/config/#glasses.config.Config","text":"Base class for configurations, all configuration must inherit from this class. For a in depth tutorial about configs, head over Configurations Important Models are not coupled with Config , therefore they are unaware of the configuration system. Each Config is linked to a specific model, not viceversa. Note A Config holds what is needed to create a model. Therefore, they are perfect to share custom version of a specific architecture. A Config is data container . Thus, it must not have any side effect , any logic that requires the Config values to be somehow processed must be implemented in the Config.build function. A custom configuration can be written as follows: from glasses.config import Config # Assume we have a model class MyModel ( nn . Module ): def __init__ ( in_channels : int , out_channels : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) def forward ( self , x ): return self . conv ( x ) # Let's create it's configuration @dataclass class MyConfig ( Config ): in_channels : int out_channels : int def build ( self ) -> nn . Module : # create a `MyModel` instance using `MyConfig` return MyModel ( ** self . __dict__ ) model : MyModel = MyConfig ( 2 , 2 ) . build () Each Config is linked to a specific model, not viceversa. Models had no idea about the configuration system and can be created normally as you expect with their constructor.","title":"Config"},{"location":"reference/config/#glasses.config.Config--nested-configurations","text":"Source code in glasses/config/__init__.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @dataclass class Config : \"\"\" Base class for configurations, all configuration **must** inherit from this class. For a in depth tutorial about configs, head over [Configurations](/) !!! important Models are not coupled with `Config`, therefore they are unaware of the configuration system. Each `Config` is linked to a specific model, not viceversa. !!! note A `Config` holds what is needed to create a model. Therefore, they are perfect to share custom version of a specific architecture. A `Config` is **data container**. Thus, it **must not have any side effect**, any logic that requires the `Config` values to be somehow processed must be implemented in the [`Config.build`](#glasses.config.Config.build) function. A custom configuration can be written as follows: ```python from glasses.config import Config # Assume we have a model class MyModel(nn.Module): def __init__(in_channels: int, out_channels: int): super().__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) def forward(self, x): return self.conv(x) # Let's create it's configuration @dataclass class MyConfig(Config): in_channels: int out_channels: int def build(self) -> nn.Module: # create a `MyModel` instance using `MyConfig` return MyModel(**self.__dict__) model: MyModel = MyConfig(2, 2).build() ``` Each Config is linked to a specific model, not viceversa. Models had no idea about the configuration system and can be created normally as you expect with their constructor. ## Nested Configurations \"\"\" def build ( self ) -> nn . Module : raise NotImplementedError def panel ( self ) -> Panel : table = Table ( title = self . __class__ . __name__ , box = box . SIMPLE , show_header = False , highlight = True , ) for key , val in self . __dict__ . items (): # if one field is a Config instance if isinstance ( val , Config ): table . add_row ( \"\" , \"\" ) table . add_row ( f \"[bold] { key } \" , val . panel ()) else : table . add_row ( key , str ( val )) return Panel ( table ) def summary ( self ): console = Console () console . print ( self . panel (), justify = \"left\" )","title":"Nested Configurations"},{"location":"reference/models/","text":"","title":"Index"},{"location":"reference/models/auto/","text":"","title":"Index"},{"location":"reference/models/auto/base/","text":"AutoModel The base AutoModel class. Usage: auto_model = AutoModel () model = auto_model . from_name ( \"my_name\" ) model = auto_model . from_pretrained ( \"my_name\" ) model = auto_model . from_pretrained ( \"my_name\" , my_config ) Source code in glasses/models/auto/base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class AutoModel : \"\"\"The base `AutoModel` class. Usage: ```python auto_model = AutoModel() model = auto_model.from_name(\"my_name\") model = auto_model.from_pretrained(\"my_name\") model = auto_model.from_pretrained(\"my_name\", my_config) ``` \"\"\" names_to_configs : Dict [ str , Callable [[], Config ]] \"\"\"Holds the map from name to config type\"\"\" @classmethod def get_config_from_name ( cls , name : str ) -> Config : return cls . names_to_configs [ name ]() @classmethod def from_name ( cls , name : str ): if name not in cls . names_to_configs : suggestions = difflib . get_close_matches ( name , cls . names_to_configs . keys ()) msg = f 'Model \" { name } \" does not exists.' if len ( suggestions ) > 0 : msg += f ' Did you mean \" { suggestions [ 0 ] } ?\"' raise KeyError ( msg ) config = cls . names_to_configs [ name ]() return config . build () @classmethod def from_pretrained ( cls , name : str , config : Optional [ Config ] = None , storage : Storage = None ) -> nn . Module : storage = LocalStorage () if storage is None else storage state_dict , loaded_config = storage . get ( name ) config = loaded_config if config is None else config model = config . build () try : model . load_state_dict ( state_dict ) except RuntimeError as e : logger . warning ( str ( e )) logger . info ( f \"Loaded pretrained weights for { name } .\" ) return model , config names_to_configs : Dict [ str , Callable [[], Config ]] class-attribute Holds the map from name to config type","title":"base"},{"location":"reference/models/auto/base/#glasses.models.auto.base.AutoModel","text":"The base AutoModel class. Usage: auto_model = AutoModel () model = auto_model . from_name ( \"my_name\" ) model = auto_model . from_pretrained ( \"my_name\" ) model = auto_model . from_pretrained ( \"my_name\" , my_config ) Source code in glasses/models/auto/base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class AutoModel : \"\"\"The base `AutoModel` class. Usage: ```python auto_model = AutoModel() model = auto_model.from_name(\"my_name\") model = auto_model.from_pretrained(\"my_name\") model = auto_model.from_pretrained(\"my_name\", my_config) ``` \"\"\" names_to_configs : Dict [ str , Callable [[], Config ]] \"\"\"Holds the map from name to config type\"\"\" @classmethod def get_config_from_name ( cls , name : str ) -> Config : return cls . names_to_configs [ name ]() @classmethod def from_name ( cls , name : str ): if name not in cls . names_to_configs : suggestions = difflib . get_close_matches ( name , cls . names_to_configs . keys ()) msg = f 'Model \" { name } \" does not exists.' if len ( suggestions ) > 0 : msg += f ' Did you mean \" { suggestions [ 0 ] } ?\"' raise KeyError ( msg ) config = cls . names_to_configs [ name ]() return config . build () @classmethod def from_pretrained ( cls , name : str , config : Optional [ Config ] = None , storage : Storage = None ) -> nn . Module : storage = LocalStorage () if storage is None else storage state_dict , loaded_config = storage . get ( name ) config = loaded_config if config is None else config model = config . build () try : model . load_state_dict ( state_dict ) except RuntimeError as e : logger . warning ( str ( e )) logger . info ( f \"Loaded pretrained weights for { name } .\" ) return model , config","title":"AutoModel"},{"location":"reference/models/auto/base/#glasses.models.auto.base.AutoModel.names_to_configs","text":"Holds the map from name to config type","title":"names_to_configs"},{"location":"reference/models/auto/model_zoo/","text":"","title":"model_zoo"},{"location":"reference/models/auto/utils/","text":"","title":"utils"},{"location":"reference/models/transform/","text":"","title":"Index"},{"location":"reference/models/transform/transform/","text":"","title":"transform"},{"location":"reference/models/vision/","text":"","title":"Index"},{"location":"reference/models/vision/auto/","text":"","title":"auto"},{"location":"reference/models/vision/backbones/","text":"","title":"Index"},{"location":"reference/models/vision/backbones/base/","text":"","title":"base"},{"location":"reference/models/vision/backbones/vit/","text":"","title":"Index"},{"location":"reference/models/vision/backbones/vit/config/","text":"","title":"config"},{"location":"reference/models/vision/backbones/vit/model/","text":"","title":"model"},{"location":"reference/models/vision/backbones/vit/zoo/","text":"","title":"zoo"},{"location":"reference/models/vision/image/","text":"","title":"Index"},{"location":"reference/models/vision/image/classification/","text":"","title":"Index"},{"location":"reference/models/vision/image/classification/base/","text":"ModelForImageClassification Bases: nn . Module Base class for classification models Define a custom image classification model. It can be anything you want, the only contrain is that it must return a ModelForImageClassificationOutput . class MyModelForImageClassification ( ModelForImageClassification ): def __init__ ( self , in_channels : int , num_classes : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , 64 , kernel_size = 3 ) self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( 64 , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : x = self . conv ( pixel_values ) x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return { \"logits\" : x } The above example is a fixed models, it doesn't have composable part. In reality, classification models are (usually) composed by a backbone and a head . Since all the backbones and heads in glasses must follow known rules, it trivial to compose them. from glasses.models.vision.backbones import ResNet class ResNetForImageClassification ( ModelForImageClassification ): def __init__ ( self , in_channels : int , ... , num_classes : int ): super () . __init__ () self . backbone = ResNet ( in_channels , .... ) self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( 64 , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : features = self . backbone ( pixel_values ) x = features [ - 1 ] x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return { \"logits\" : x } In 99% of cases you will take advantage of the AnyModelForImageClassification that allows you to mix on the fly any backbone and classification head in glasses. Source code in glasses/models/vision/image/classification/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class ModelForImageClassification ( nn . Module ): \"\"\"Base class for classification models Define a custom image classification model. It can be anything you want, the only contrain is that it **must** return a `ModelForImageClassificationOutput`. ```python class MyModelForImageClassification(ModelForImageClassification): def __init__(self, in_channels: int, num_classes: int ): super().__init__() self.conv = nn.Conv2d(in_channels, 64, kernel_size=3) self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(64, num_classes) def forward(self, pixel_values: Tensor) -> ModelForImageClassificationOutput: x = self.conv(pixel_values) x = self.pool(x) x = self.flat(x) x = self.fc(x) return {\"logits\": x} ``` The above example is a fixed models, it doesn't have composable part. In reality, classification models are (usually) composed by a **backbone** and a **head**. Since all the backbones and heads in glasses must follow known rules, it trivial to compose them. ```python from glasses.models.vision.backbones import ResNet class ResNetForImageClassification(ModelForImageClassification): def __init__(self, in_channels: int, ..., num_classes: int): super().__init__() self.backbone =ResNet(in_channels, ....) self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(64, num_classes) def forward(self, pixel_values: Tensor) -> ModelForImageClassificationOutput: features = self.backbone(pixel_values) x = features[-1] x = self.pool(x) x = self.flat(x) x = self.fc(x) return {\"logits\": x} ``` In 99% of cases you will take advantage of the [`AnyModelForImageClassification`]() that allows you to mix on the fly any backbone and classification head in glasses. \"\"\" def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : \"\"\"The forward method for classification head. Args: pixel_values (Tensor): The input image. Returns: Tensor: The logits. \"\"\" raise NotImplementedError forward ( pixel_values ) The forward method for classification head. Parameters: Name Type Description Default pixel_values Tensor The input image. required Returns: Name Type Description Tensor ModelForImageClassificationOutput The logits. Source code in glasses/models/vision/image/classification/base.py 59 60 61 62 63 64 65 66 67 68 def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : \"\"\"The forward method for classification head. Args: pixel_values (Tensor): The input image. Returns: Tensor: The logits. \"\"\" raise NotImplementedError","title":"base"},{"location":"reference/models/vision/image/classification/base/#glasses.models.vision.image.classification.base.ModelForImageClassification","text":"Bases: nn . Module Base class for classification models Define a custom image classification model. It can be anything you want, the only contrain is that it must return a ModelForImageClassificationOutput . class MyModelForImageClassification ( ModelForImageClassification ): def __init__ ( self , in_channels : int , num_classes : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , 64 , kernel_size = 3 ) self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( 64 , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : x = self . conv ( pixel_values ) x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return { \"logits\" : x } The above example is a fixed models, it doesn't have composable part. In reality, classification models are (usually) composed by a backbone and a head . Since all the backbones and heads in glasses must follow known rules, it trivial to compose them. from glasses.models.vision.backbones import ResNet class ResNetForImageClassification ( ModelForImageClassification ): def __init__ ( self , in_channels : int , ... , num_classes : int ): super () . __init__ () self . backbone = ResNet ( in_channels , .... ) self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( 64 , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : features = self . backbone ( pixel_values ) x = features [ - 1 ] x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return { \"logits\" : x } In 99% of cases you will take advantage of the AnyModelForImageClassification that allows you to mix on the fly any backbone and classification head in glasses. Source code in glasses/models/vision/image/classification/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class ModelForImageClassification ( nn . Module ): \"\"\"Base class for classification models Define a custom image classification model. It can be anything you want, the only contrain is that it **must** return a `ModelForImageClassificationOutput`. ```python class MyModelForImageClassification(ModelForImageClassification): def __init__(self, in_channels: int, num_classes: int ): super().__init__() self.conv = nn.Conv2d(in_channels, 64, kernel_size=3) self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(64, num_classes) def forward(self, pixel_values: Tensor) -> ModelForImageClassificationOutput: x = self.conv(pixel_values) x = self.pool(x) x = self.flat(x) x = self.fc(x) return {\"logits\": x} ``` The above example is a fixed models, it doesn't have composable part. In reality, classification models are (usually) composed by a **backbone** and a **head**. Since all the backbones and heads in glasses must follow known rules, it trivial to compose them. ```python from glasses.models.vision.backbones import ResNet class ResNetForImageClassification(ModelForImageClassification): def __init__(self, in_channels: int, ..., num_classes: int): super().__init__() self.backbone =ResNet(in_channels, ....) self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(64, num_classes) def forward(self, pixel_values: Tensor) -> ModelForImageClassificationOutput: features = self.backbone(pixel_values) x = features[-1] x = self.pool(x) x = self.flat(x) x = self.fc(x) return {\"logits\": x} ``` In 99% of cases you will take advantage of the [`AnyModelForImageClassification`]() that allows you to mix on the fly any backbone and classification head in glasses. \"\"\" def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : \"\"\"The forward method for classification head. Args: pixel_values (Tensor): The input image. Returns: Tensor: The logits. \"\"\" raise NotImplementedError","title":"ModelForImageClassification"},{"location":"reference/models/vision/image/classification/base/#glasses.models.vision.image.classification.base.ModelForImageClassification.forward","text":"The forward method for classification head. Parameters: Name Type Description Default pixel_values Tensor The input image. required Returns: Name Type Description Tensor ModelForImageClassificationOutput The logits. Source code in glasses/models/vision/image/classification/base.py 59 60 61 62 63 64 65 66 67 68 def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : \"\"\"The forward method for classification head. Args: pixel_values (Tensor): The input image. Returns: Tensor: The logits. \"\"\" raise NotImplementedError","title":"forward()"},{"location":"reference/models/vision/image/classification/outputs/","text":"ModelForImageClassificationOutput Bases: TypedDict The output for image classification models. Source code in glasses/models/vision/image/classification/outputs.py 6 7 8 9 class ModelForImageClassificationOutput ( TypedDict ): \"\"\"The output for image classification models.\"\"\" logits : Tensor","title":"outputs"},{"location":"reference/models/vision/image/classification/outputs/#glasses.models.vision.image.classification.outputs.ModelForImageClassificationOutput","text":"Bases: TypedDict The output for image classification models. Source code in glasses/models/vision/image/classification/outputs.py 6 7 8 9 class ModelForImageClassificationOutput ( TypedDict ): \"\"\"The output for image classification models.\"\"\" logits : Tensor","title":"ModelForImageClassificationOutput"},{"location":"reference/models/vision/image/classification/common/","text":"","title":"Index"},{"location":"reference/models/vision/image/classification/common/config/","text":"","title":"config"},{"location":"reference/models/vision/image/classification/common/model/","text":"","title":"model"},{"location":"reference/models/vision/image/classification/heads/","text":"","title":"Index"},{"location":"reference/models/vision/image/classification/heads/base/","text":"HeadForImageClassification Bases: nn . Module Base class for classification heads Define a custom classification head class LinearHead ( HeadForImageClassification ): def __init__ ( self , num_classes : int , in_channels : int ): super () . __init__ () self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( in_channels , num_classes ) def forward ( self , features : List [ Tensor ]) -> Tensor : x = features [ - 1 ] x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return x Source code in glasses/models/vision/image/classification/heads/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class HeadForImageClassification ( nn . Module ): \"\"\"Base class for classification heads Define a custom classification head ```python class LinearHead(HeadForImageClassification): def __init__(self, num_classes: int, in_channels: int): super().__init__() self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(in_channels, num_classes) def forward(self, features: List[Tensor]) -> Tensor: x = features[-1] x = self.pool(x) x = self.flat(x) x = self.fc(x) return x ``` \"\"\" def forward ( self , features : List [ Tensor ]) -> Tensor : \"\"\"The forward method for classification head. Args: features (List[Tensor]): A list of features. Returns: Tensor: The logits \"\"\" raise NotImplementedError forward ( features ) The forward method for classification head. Parameters: Name Type Description Default features List [ Tensor ] A list of features. required Returns: Name Type Description Tensor Tensor The logits Source code in glasses/models/vision/image/classification/heads/base.py 30 31 32 33 34 35 36 37 38 39 def forward ( self , features : List [ Tensor ]) -> Tensor : \"\"\"The forward method for classification head. Args: features (List[Tensor]): A list of features. Returns: Tensor: The logits \"\"\" raise NotImplementedError","title":"base"},{"location":"reference/models/vision/image/classification/heads/base/#glasses.models.vision.image.classification.heads.base.HeadForImageClassification","text":"Bases: nn . Module Base class for classification heads Define a custom classification head class LinearHead ( HeadForImageClassification ): def __init__ ( self , num_classes : int , in_channels : int ): super () . __init__ () self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( in_channels , num_classes ) def forward ( self , features : List [ Tensor ]) -> Tensor : x = features [ - 1 ] x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return x Source code in glasses/models/vision/image/classification/heads/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class HeadForImageClassification ( nn . Module ): \"\"\"Base class for classification heads Define a custom classification head ```python class LinearHead(HeadForImageClassification): def __init__(self, num_classes: int, in_channels: int): super().__init__() self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(in_channels, num_classes) def forward(self, features: List[Tensor]) -> Tensor: x = features[-1] x = self.pool(x) x = self.flat(x) x = self.fc(x) return x ``` \"\"\" def forward ( self , features : List [ Tensor ]) -> Tensor : \"\"\"The forward method for classification head. Args: features (List[Tensor]): A list of features. Returns: Tensor: The logits \"\"\" raise NotImplementedError","title":"HeadForImageClassification"},{"location":"reference/models/vision/image/classification/heads/base/#glasses.models.vision.image.classification.heads.base.HeadForImageClassification.forward","text":"The forward method for classification head. Parameters: Name Type Description Default features List [ Tensor ] A list of features. required Returns: Name Type Description Tensor Tensor The logits Source code in glasses/models/vision/image/classification/heads/base.py 30 31 32 33 34 35 36 37 38 39 def forward ( self , features : List [ Tensor ]) -> Tensor : \"\"\"The forward method for classification head. Args: features (List[Tensor]): A list of features. Returns: Tensor: The logits \"\"\" raise NotImplementedError","title":"forward()"},{"location":"reference/models/vision/image/classification/heads/linear_head/","text":"","title":"Index"},{"location":"reference/models/vision/image/classification/heads/linear_head/config/","text":"","title":"config"},{"location":"reference/models/vision/image/classification/heads/linear_head/model/","text":"","title":"model"},{"location":"reference/models/vision/image/classification/heads/vit/","text":"","title":"Index"},{"location":"reference/models/vision/image/classification/heads/vit/config/","text":"","title":"config"},{"location":"reference/models/vision/image/classification/heads/vit/model/","text":"ViTHead Bases: HeadForImageClassification Source code in glasses/models/vision/image/classification/heads/vit/model.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class ViTHead ( HeadForImageClassification ): POLICIES = [ \"token\" , \"mean\" ] def __init__ ( self , emb_size : int = 768 , num_classes : int = 1000 , policy : str = \"token\" ): \"\"\" ViT Classification Head Args: emb_size (int, optional): Embedding dimensions Defaults to 768. num_classes (int, optional): [description]. Defaults to 1000. policy (str, optional): Pooling policy, can be token or mean. Defaults to 'token'. \"\"\" if policy not in self . POLICIES : raise ValueError ( f \"Only policies { ',' . join ( self . POLICIES ) } are supported\" ) super () . __init__ () self . pool = ( Reduce ( \"b n e -> b e\" , reduction = \"mean\" ) if policy == \"mean\" else Lambda ( lambda x : x [:, 0 ]) ) self . fc = nn . Linear ( emb_size , num_classes ) def forward ( self , features : List [ Tensor ]) -> Tensor : x = self . pool ( features [ - 1 ]) x = self . fc ( x ) return x __init__ ( emb_size = 768 , num_classes = 1000 , policy = 'token' ) ViT Classification Head Parameters: Name Type Description Default emb_size int Embedding dimensions Defaults to 768. 768 num_classes int [description]. Defaults to 1000. 1000 policy str Pooling policy, can be token or mean. Defaults to 'token'. 'token' Source code in glasses/models/vision/image/classification/heads/vit/model.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , emb_size : int = 768 , num_classes : int = 1000 , policy : str = \"token\" ): \"\"\" ViT Classification Head Args: emb_size (int, optional): Embedding dimensions Defaults to 768. num_classes (int, optional): [description]. Defaults to 1000. policy (str, optional): Pooling policy, can be token or mean. Defaults to 'token'. \"\"\" if policy not in self . POLICIES : raise ValueError ( f \"Only policies { ',' . join ( self . POLICIES ) } are supported\" ) super () . __init__ () self . pool = ( Reduce ( \"b n e -> b e\" , reduction = \"mean\" ) if policy == \"mean\" else Lambda ( lambda x : x [:, 0 ]) ) self . fc = nn . Linear ( emb_size , num_classes )","title":"model"},{"location":"reference/models/vision/image/classification/heads/vit/model/#glasses.models.vision.image.classification.heads.vit.model.ViTHead","text":"Bases: HeadForImageClassification Source code in glasses/models/vision/image/classification/heads/vit/model.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class ViTHead ( HeadForImageClassification ): POLICIES = [ \"token\" , \"mean\" ] def __init__ ( self , emb_size : int = 768 , num_classes : int = 1000 , policy : str = \"token\" ): \"\"\" ViT Classification Head Args: emb_size (int, optional): Embedding dimensions Defaults to 768. num_classes (int, optional): [description]. Defaults to 1000. policy (str, optional): Pooling policy, can be token or mean. Defaults to 'token'. \"\"\" if policy not in self . POLICIES : raise ValueError ( f \"Only policies { ',' . join ( self . POLICIES ) } are supported\" ) super () . __init__ () self . pool = ( Reduce ( \"b n e -> b e\" , reduction = \"mean\" ) if policy == \"mean\" else Lambda ( lambda x : x [:, 0 ]) ) self . fc = nn . Linear ( emb_size , num_classes ) def forward ( self , features : List [ Tensor ]) -> Tensor : x = self . pool ( features [ - 1 ]) x = self . fc ( x ) return x","title":"ViTHead"},{"location":"reference/models/vision/image/classification/heads/vit/model/#glasses.models.vision.image.classification.heads.vit.model.ViTHead.__init__","text":"ViT Classification Head Parameters: Name Type Description Default emb_size int Embedding dimensions Defaults to 768. 768 num_classes int [description]. Defaults to 1000. 1000 policy str Pooling policy, can be token or mean. Defaults to 'token'. 'token' Source code in glasses/models/vision/image/classification/heads/vit/model.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , emb_size : int = 768 , num_classes : int = 1000 , policy : str = \"token\" ): \"\"\" ViT Classification Head Args: emb_size (int, optional): Embedding dimensions Defaults to 768. num_classes (int, optional): [description]. Defaults to 1000. policy (str, optional): Pooling policy, can be token or mean. Defaults to 'token'. \"\"\" if policy not in self . POLICIES : raise ValueError ( f \"Only policies { ',' . join ( self . POLICIES ) } are supported\" ) super () . __init__ () self . pool = ( Reduce ( \"b n e -> b e\" , reduction = \"mean\" ) if policy == \"mean\" else Lambda ( lambda x : x [:, 0 ]) ) self . fc = nn . Linear ( emb_size , num_classes )","title":"__init__()"},{"location":"reference/models/vision/image/classification/vit/","text":"","title":"Index"},{"location":"reference/models/vision/image/classification/vit/config/","text":"ViTForImageClassificationConfig dataclass Bases: AnyModelForImageClassificationConfig Config for ViT model Source code in glasses/models/vision/image/classification/vit/config.py 9 10 11 12 13 14 @dataclass class ViTForImageClassificationConfig ( AnyModelForImageClassificationConfig ): \"\"\"Config for [`ViT`](/models/vision/image/classification/vit) model\"\"\" backbone_config : ViTBackboneConfig head_config : ViTHeadConfig","title":"config"},{"location":"reference/models/vision/image/classification/vit/config/#glasses.models.vision.image.classification.vit.config.ViTForImageClassificationConfig","text":"Bases: AnyModelForImageClassificationConfig Config for ViT model Source code in glasses/models/vision/image/classification/vit/config.py 9 10 11 12 13 14 @dataclass class ViTForImageClassificationConfig ( AnyModelForImageClassificationConfig ): \"\"\"Config for [`ViT`](/models/vision/image/classification/vit) model\"\"\" backbone_config : ViTBackboneConfig head_config : ViTHeadConfig","title":"ViTForImageClassificationConfig"},{"location":"reference/models/vision/image/classification/vit/zoo/","text":"","title":"zoo"},{"location":"reference/models/vision/image/detection/","text":"","title":"Index"},{"location":"reference/models/vision/image/detection/outputs/","text":"ModelForImageDetectionOutput Bases: TypedDict The output for image detection models. Source code in glasses/models/vision/image/detection/outputs.py 6 7 8 9 10 11 12 class ModelForImageDetectionOutput ( TypedDict ): \"\"\"The output for image detection models.\"\"\" logits : Tensor \"\"\"A `torch.Tensor` of shape `(batch_size, num_bboxes, num_classes + 1)`.\"\"\" bboxes : Tensor \"\"\"A `torch.Tensor` of shape `(batch_size, num_bboxes, 4)`.\"\"\" bboxes : Tensor class-attribute A torch.Tensor of shape (batch_size, num_bboxes, 4) . logits : Tensor class-attribute A torch.Tensor of shape (batch_size, num_bboxes, num_classes + 1) .","title":"outputs"},{"location":"reference/models/vision/image/detection/outputs/#glasses.models.vision.image.detection.outputs.ModelForImageDetectionOutput","text":"Bases: TypedDict The output for image detection models. Source code in glasses/models/vision/image/detection/outputs.py 6 7 8 9 10 11 12 class ModelForImageDetectionOutput ( TypedDict ): \"\"\"The output for image detection models.\"\"\" logits : Tensor \"\"\"A `torch.Tensor` of shape `(batch_size, num_bboxes, num_classes + 1)`.\"\"\" bboxes : Tensor \"\"\"A `torch.Tensor` of shape `(batch_size, num_bboxes, 4)`.\"\"\"","title":"ModelForImageDetectionOutput"},{"location":"reference/models/vision/image/detection/outputs/#glasses.models.vision.image.detection.outputs.ModelForImageDetectionOutput.bboxes","text":"A torch.Tensor of shape (batch_size, num_bboxes, 4) .","title":"bboxes"},{"location":"reference/models/vision/image/detection/outputs/#glasses.models.vision.image.detection.outputs.ModelForImageDetectionOutput.logits","text":"A torch.Tensor of shape (batch_size, num_bboxes, num_classes + 1) .","title":"logits"},{"location":"reference/models/vision/image/segmentation/","text":"","title":"Index"},{"location":"reference/models/vision/image/segmentation/outputs/","text":"ModelForImageSegmentationOutput Bases: TypedDict The output for image segmentation models. Source code in glasses/models/vision/image/segmentation/outputs.py 7 8 9 10 11 12 13 14 15 class ModelForImageSegmentationOutput ( TypedDict ): \"\"\"The output for image segmentation models.\"\"\" pixel_logits : Tensor \"\"\"A `torch.Tensor` of shape `(batch_size, num_classes, height, width)`. !!! note The `height` and `width` are usually smaller than the original image. \"\"\" pixel_logits : Tensor class-attribute A torch.Tensor of shape (batch_size, num_classes, height, width) . Note The height and width are usually smaller than the original image.","title":"outputs"},{"location":"reference/models/vision/image/segmentation/outputs/#glasses.models.vision.image.segmentation.outputs.ModelForImageSegmentationOutput","text":"Bases: TypedDict The output for image segmentation models. Source code in glasses/models/vision/image/segmentation/outputs.py 7 8 9 10 11 12 13 14 15 class ModelForImageSegmentationOutput ( TypedDict ): \"\"\"The output for image segmentation models.\"\"\" pixel_logits : Tensor \"\"\"A `torch.Tensor` of shape `(batch_size, num_classes, height, width)`. !!! note The `height` and `width` are usually smaller than the original image. \"\"\"","title":"ModelForImageSegmentationOutput"},{"location":"reference/models/vision/image/segmentation/outputs/#glasses.models.vision.image.segmentation.outputs.ModelForImageSegmentationOutput.pixel_logits","text":"A torch.Tensor of shape (batch_size, num_classes, height, width) . Note The height and width are usually smaller than the original image.","title":"pixel_logits"},{"location":"reference/models/vision/necks/","text":"","title":"Index"},{"location":"reference/models/vision/necks/base/","text":"","title":"base"},{"location":"reference/nn/","text":"Lambda Bases: nn . Module Source code in glasses/nn/__init__.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Lambda ( nn . Module ): def __init__ ( self , lambd : Callable [[ Tensor ], Tensor ]): \"\"\"An utility Module, it allows custom function to be passed Args: lambd (Callable[Tensor]): A function that does something on a tensor Usage: ```python add_two = Lambda(lambd x: x + 2) add_two(Tensor([0])) // 2 ``` \"\"\" super () . __init__ () self . lambd = lambd def forward ( self , x : Tensor ) -> Tensor : return self . lambd ( x ) __init__ ( lambd ) An utility Module, it allows custom function to be passed Parameters: Name Type Description Default lambd Callable [ Tensor ] A function that does something on a tensor required Usage: add_two = Lambda ( lambd x : x + 2 ) add_two ( Tensor ([ 0 ])) // 2 Source code in glasses/nn/__init__.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , lambd : Callable [[ Tensor ], Tensor ]): \"\"\"An utility Module, it allows custom function to be passed Args: lambd (Callable[Tensor]): A function that does something on a tensor Usage: ```python add_two = Lambda(lambd x: x + 2) add_two(Tensor([0])) // 2 ``` \"\"\" super () . __init__ () self . lambd = lambd","title":"nn"},{"location":"reference/nn/#glasses.nn.Lambda","text":"Bases: nn . Module Source code in glasses/nn/__init__.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Lambda ( nn . Module ): def __init__ ( self , lambd : Callable [[ Tensor ], Tensor ]): \"\"\"An utility Module, it allows custom function to be passed Args: lambd (Callable[Tensor]): A function that does something on a tensor Usage: ```python add_two = Lambda(lambd x: x + 2) add_two(Tensor([0])) // 2 ``` \"\"\" super () . __init__ () self . lambd = lambd def forward ( self , x : Tensor ) -> Tensor : return self . lambd ( x )","title":"Lambda"},{"location":"reference/nn/#glasses.nn.Lambda.__init__","text":"An utility Module, it allows custom function to be passed Parameters: Name Type Description Default lambd Callable [ Tensor ] A function that does something on a tensor required Usage: add_two = Lambda ( lambd x : x + 2 ) add_two ( Tensor ([ 0 ])) // 2 Source code in glasses/nn/__init__.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , lambd : Callable [[ Tensor ], Tensor ]): \"\"\"An utility Module, it allows custom function to be passed Args: lambd (Callable[Tensor]): A function that does something on a tensor Usage: ```python add_two = Lambda(lambd x: x + 2) add_two(Tensor([0])) // 2 ``` \"\"\" super () . __init__ () self . lambd = lambd","title":"__init__()"},{"location":"reference/storage/","text":"","title":"Index"},{"location":"reference/storage/base/","text":"","title":"base"},{"location":"reference/storage/huggingface/","text":"","title":"huggingface"},{"location":"reference/storage/local/","text":"","title":"local"},{"location":"reference/utils/","text":"","title":"Index"},{"location":"reference/utils/model_tester/","text":"model_tester ( config , input_dict , output_shape_dict , output_dict_type , output_test_strategy = None , output_equivalence_dict = None ) This functions tests the mdoel using different checks. In order: check that model's outputs shapes matched output_shape_dict check that the model can be trained, we use output_dict_type to know how to run output's specific tests. For example, for image classification you should pass output_dict_type=ModelForImageClassificationOutput and we will try to run outputs[\"logits\"].mean().backward() optionally, if output_equivalence_dict we will check that model's outputs match Parameters: Name Type Description Default config Config Configuration we will use to build the model. required input_dict Dict [ str , Tensor ] Dictionary containing the inputs for the model. E.g. { \"pixel_values\" : torch.randn((1, 3, 224, 224))} required output_shape_dict Dict [ str , Tuple [ int ]] Dictionary containing the expected shaped in the output. E.g. {\"logits\": (1, 1000)} required output_dict_type TypedDict The type of output, e.g. ModelForImageClassificationOutput](). Used to now which test strategy to run, based on the type we know how to test it. Defaults to None. required output_test_strategy Optional [ Callable [[ TypedDict ]]] If passed, we will use this strategy instead. None output_equivalence_dict Optional [ Dict [ str , Tensor ]] If passes, we will check that the model's output are equal to the values inside it. Defaults to None. None Source code in glasses/utils/model_tester.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def model_tester ( config : Config , input_dict : Dict [ str , Tensor ], output_shape_dict : Dict [ str , Tuple [ int ]], output_dict_type : TypedDict , output_test_strategy : Optional [ Callable [[ TypedDict ], None ]] = None , output_equivalence_dict : Optional [ Dict [ str , Tensor ]] = None , ): \"\"\" This functions tests the mdoel using different checks. In order: - check that model's outputs shapes matched `output_shape_dict` - check that the model can be trained, we use `output_dict_type` to know how to run output's specific tests. For example, for image classification you should pass `output_dict_type=ModelForImageClassificationOutput` and we will try to run `outputs[\"logits\"].mean().backward()` - optionally, if `output_equivalence_dict` we will check that model's outputs match Args: config (Config): Configuration we will use to build the model. input_dict (Dict[str, Tensor]): Dictionary containing the inputs for the model. E.g. `{ \"pixel_values\" : torch.randn((1, 3, 224, 224))}` output_shape_dict (Dict[str, Tuple[int]]): Dictionary containing the expected shaped in the output. E.g. ` {\"logits\": (1, 1000)}` output_dict_type (TypedDict): The type of output, e.g. ModelForImageClassificationOutput](). Used to now which test strategy to run, based on the type we know how to test it. Defaults to None. output_test_strategy (Optional[Callable[[TypedDict]]], optional): If passed, we will use this strategy instead. output_equivalence_dict (Optional[Dict[str, Tensor]], optional): If passes, we will check that the model's output are equal to the values inside it. Defaults to None. \"\"\" with torch . no_grad (): # we are able to create the model from a config model = config . build () . eval () model_output_dict = model ( ** input_dict ) check_model_output_dict_shape ( model_output_dict , output_shape_dict ) model = model . train () model_output_dict = model ( ** input_dict ) # we are able to check that the model can be trained output_test_strategy = output_test_strategy if output_test_strategy is None : output_test_strategy = model_output_test_strategies [ output_dict_type ] output_test_strategy ( model_output_dict ) check_grad_exists ( model ) # we are able to check outputs equivalence if passed if output_equivalence_dict : with torch . no_grad (): # here run the model with a known input check_output_equivalence ( model_output_dict , output_equivalence_dict )","title":"model_tester"},{"location":"reference/utils/model_tester/#glasses.utils.model_tester.model_tester","text":"This functions tests the mdoel using different checks. In order: check that model's outputs shapes matched output_shape_dict check that the model can be trained, we use output_dict_type to know how to run output's specific tests. For example, for image classification you should pass output_dict_type=ModelForImageClassificationOutput and we will try to run outputs[\"logits\"].mean().backward() optionally, if output_equivalence_dict we will check that model's outputs match Parameters: Name Type Description Default config Config Configuration we will use to build the model. required input_dict Dict [ str , Tensor ] Dictionary containing the inputs for the model. E.g. { \"pixel_values\" : torch.randn((1, 3, 224, 224))} required output_shape_dict Dict [ str , Tuple [ int ]] Dictionary containing the expected shaped in the output. E.g. {\"logits\": (1, 1000)} required output_dict_type TypedDict The type of output, e.g. ModelForImageClassificationOutput](). Used to now which test strategy to run, based on the type we know how to test it. Defaults to None. required output_test_strategy Optional [ Callable [[ TypedDict ]]] If passed, we will use this strategy instead. None output_equivalence_dict Optional [ Dict [ str , Tensor ]] If passes, we will check that the model's output are equal to the values inside it. Defaults to None. None Source code in glasses/utils/model_tester.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def model_tester ( config : Config , input_dict : Dict [ str , Tensor ], output_shape_dict : Dict [ str , Tuple [ int ]], output_dict_type : TypedDict , output_test_strategy : Optional [ Callable [[ TypedDict ], None ]] = None , output_equivalence_dict : Optional [ Dict [ str , Tensor ]] = None , ): \"\"\" This functions tests the mdoel using different checks. In order: - check that model's outputs shapes matched `output_shape_dict` - check that the model can be trained, we use `output_dict_type` to know how to run output's specific tests. For example, for image classification you should pass `output_dict_type=ModelForImageClassificationOutput` and we will try to run `outputs[\"logits\"].mean().backward()` - optionally, if `output_equivalence_dict` we will check that model's outputs match Args: config (Config): Configuration we will use to build the model. input_dict (Dict[str, Tensor]): Dictionary containing the inputs for the model. E.g. `{ \"pixel_values\" : torch.randn((1, 3, 224, 224))}` output_shape_dict (Dict[str, Tuple[int]]): Dictionary containing the expected shaped in the output. E.g. ` {\"logits\": (1, 1000)}` output_dict_type (TypedDict): The type of output, e.g. ModelForImageClassificationOutput](). Used to now which test strategy to run, based on the type we know how to test it. Defaults to None. output_test_strategy (Optional[Callable[[TypedDict]]], optional): If passed, we will use this strategy instead. output_equivalence_dict (Optional[Dict[str, Tensor]], optional): If passes, we will check that the model's output are equal to the values inside it. Defaults to None. \"\"\" with torch . no_grad (): # we are able to create the model from a config model = config . build () . eval () model_output_dict = model ( ** input_dict ) check_model_output_dict_shape ( model_output_dict , output_shape_dict ) model = model . train () model_output_dict = model ( ** input_dict ) # we are able to check that the model can be trained output_test_strategy = output_test_strategy if output_test_strategy is None : output_test_strategy = model_output_test_strategies [ output_dict_type ] output_test_strategy ( model_output_dict ) check_grad_exists ( model ) # we are able to check outputs equivalence if passed if output_equivalence_dict : with torch . no_grad (): # here run the model with a known input check_output_equivalence ( model_output_dict , output_equivalence_dict )","title":"model_tester()"},{"location":"tutorial/coding-style/","text":"In order to properly create a truly amazing codebase, we must agree on some conding conventions. We follow the PEP 8 style guide for Python Code. Naming Convention Names We are lazy programmers! Keep the variables names short but meaningfull: convolution -> conv activation -> act linear/dense -> fc batchnorm -> bn ... etc Models Block : we refer to Block to mean a minimum building block of a model. Stage : a Stage is a collection of Block s A model is always linked to a task. Therefore, each model follows the <ModelName>For<TaskName> naming convention. FOr example, ResNetForImageClassification . When you have something very simple, you can directly subclass nn.Sequential to avoid writing a trivial forward function. # This module is trivial, use nn.Sequential instead! class ConvBnReLU ( nn . Module ): def __init__ ( self , ... ): super () . __init__ () self . conv = nn . Conv2d ( ... ) self . bn = nn . BatchNorm2d ( ... ) self . relu = nn . ReLU () def forward ( self , x ): x = self . conv ( x ) x = self . bn ( x ) x = self . relu ( x ) return x # no need for the forward class ConvBnReLU ( nn . Sequential ): def __init__ ( self , ... ): super () . __init__ () self . conv = nn . Conv2d ( ... ) self . bn = nn . BatchNorm2d ( ... ) self . relu = nn . ReLU () Design We follow the single responsability principle where each class/function does only one thing. For example, assume we have the following model (we are subclassing nn.Sequential for simplicity). class MyModel ( nn . Module ): def __init__ ( ... ): super () . __init__ () self . embedder = nn . Sequential ( nn . Conv2d ( .... ), nn . BatchNorm2d ( ... ), nn . ReLU () ) self . encoder = nn . Sequential ( nn . Conv2d ( ... ), nn . Conv2d ( ... ), nn . Conv2d ( ... ) ) self . head = nn . Sequential ( nn . Linear ( ... ) ) class MyModelEmbedder ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Conv2d ( ... ), nn . BatchNorm2d ( ... ), nn . ReLU () ) class MyModelEncoder ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Conv2d ( ... ), nn . Conv2d ( ... ), nn . Conv2d ( ... ) ) class MyModelHead ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Linear ( ... )) class MyModel ( nn . Module ): def __init__ ( ... ): super () . __init__ () self . embedder = MyModelEmbedder ( ... ) self . encoder = MyModelEncoder ( ... ) self . head = MyModelHead ( ... ) Each module does only one thing. This makes it easier to document and share each individual parts.","title":"Coding style"},{"location":"tutorial/coding-style/#naming-convention","text":"","title":"Naming Convention"},{"location":"tutorial/coding-style/#names","text":"We are lazy programmers! Keep the variables names short but meaningfull: convolution -> conv activation -> act linear/dense -> fc batchnorm -> bn ... etc","title":"Names"},{"location":"tutorial/coding-style/#models","text":"Block : we refer to Block to mean a minimum building block of a model. Stage : a Stage is a collection of Block s A model is always linked to a task. Therefore, each model follows the <ModelName>For<TaskName> naming convention. FOr example, ResNetForImageClassification . When you have something very simple, you can directly subclass nn.Sequential to avoid writing a trivial forward function. # This module is trivial, use nn.Sequential instead! class ConvBnReLU ( nn . Module ): def __init__ ( self , ... ): super () . __init__ () self . conv = nn . Conv2d ( ... ) self . bn = nn . BatchNorm2d ( ... ) self . relu = nn . ReLU () def forward ( self , x ): x = self . conv ( x ) x = self . bn ( x ) x = self . relu ( x ) return x # no need for the forward class ConvBnReLU ( nn . Sequential ): def __init__ ( self , ... ): super () . __init__ () self . conv = nn . Conv2d ( ... ) self . bn = nn . BatchNorm2d ( ... ) self . relu = nn . ReLU ()","title":"Models"},{"location":"tutorial/coding-style/#design","text":"We follow the single responsability principle where each class/function does only one thing. For example, assume we have the following model (we are subclassing nn.Sequential for simplicity). class MyModel ( nn . Module ): def __init__ ( ... ): super () . __init__ () self . embedder = nn . Sequential ( nn . Conv2d ( .... ), nn . BatchNorm2d ( ... ), nn . ReLU () ) self . encoder = nn . Sequential ( nn . Conv2d ( ... ), nn . Conv2d ( ... ), nn . Conv2d ( ... ) ) self . head = nn . Sequential ( nn . Linear ( ... ) ) class MyModelEmbedder ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Conv2d ( ... ), nn . BatchNorm2d ( ... ), nn . ReLU () ) class MyModelEncoder ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Conv2d ( ... ), nn . Conv2d ( ... ), nn . Conv2d ( ... ) ) class MyModelHead ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Linear ( ... )) class MyModel ( nn . Module ): def __init__ ( ... ): super () . __init__ () self . embedder = MyModelEmbedder ( ... ) self . encoder = MyModelEncoder ( ... ) self . head = MyModelHead ( ... ) Each module does only one thing. This makes it easier to document and share each individual parts.","title":"Design"},{"location":"tutorial/configurations/","text":"Glasses uses a configuration system to record/share and load custom versions of a specific architecture. Note Configurations in glasses are python dataclasses , thus at any point in time, you can import them and know exactly what goes inside. The main idea behind our configuration system is to be an addition to the models, not a requirement . Any model in classes can be created by just importing it and passing the right parameters, they don't know about configurations. Saying that, why do we need configurations? Configurations are necessary when we need to store a specific set of parameters for a model. For example, if a model was trained on dataset X with ten classes, our configuration will contain all the parameters need to create that specific model. In most libraries, configurations are serialized files (e.g. yml ), in glasses, they are code. This allows the user to take advantage of its IDE and see the parameters at any point in time. Basic Config Let's see how to create a basic config. First, we need a model from torch import nn class MyModel ( nn . Module ): def __init__ ( in_channels : int , out_channels : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) def forward ( self , x ): return self . conv ( x ) Then we can create it's configuration from glasses.config import Config # Let's create it's configuration @dataclass class MyConfig ( Config ): in_channels : int out_channels : int def build ( self ) -> nn . Module : # create a `MyModel` instance using `MyConfig` return MyModel ( ** self . __dict__ ) We can now invoke the build method, which will create the model model : MyModel = MyConfig ( 2 , 2 ) . build () # same as model : MyModel = MyModel ( 2 , 2 ) Nothing very special. Nested Config Let's see how to create a nested config . Assume we have a model that takes a backbone and has a fixed head. from torch import nn class MyModel ( nn . Module ): def __init__ ( self , backbone : nn . Module , channels : int , num_classes : int ): super () . __init__ () self . backbone = backbone self . head = nn . Conv2d ( channels , num_classes , kernel_size = 1 ) def forward ( self , x ): features = self . backbone ( x ) out = self . head ( features [ - 1 ]) # use last feature return out Our config will be nested since backbone has its own configuration. from glasses.config import Config @dataclass class MyConfig ( Config ): backbone_config : Config channels : int num_classes : int def build ( self ) -> nn . Module : backbone = backbone_config . build () # create a `MyModel` instance using `MyConfig` return MyModel ( backbone , self . channels , self . num_classes ) As expected, we must have configs for the different backbones we want to use. from torch import nn from glasses.config import Config class BackboneA ( nn . Module ): def __init__ ( ... ): ... @dataclass class BackboneAConfig ( Config ): ... class BackboneB ( nn . Module ): def __init__ ( ... ): .... @dataclass class BackboneBConfig ( Config ): ... Then, we can pass any backbone to MyConfig . config = MyConfig ( backbone_config = BackboneAConfig ( ... ), ... ) config . build () # build model with backbone A config = MyConfig ( backbone_config = BackboneBConfig ( ... ), ... ) config . build () # build model with backbone B The main advantage of the config system is when we need to save a specific model version. For instance, assume I have trained MyModel with BackboneA on dataset X . Its config will look like this: my_model_backbone_a_x = MyConfig ( backbone_config = BackboneAConfig ( ... ), channels = 64 , num_classes = 10 ) Therefore, at any point in time, I can recreate the model and load its pretrained weights. my_model_backbone_a_x . build () . load_state_dict ( \"/somewhere/my_model_backbone_a_x.pth\" ) Now, what if I want to use my_model_backbone_a_x architecture but just change a small part? Maybe the number of classes? # clone the config config = MyConfig ( ** my_model_backbone_a_x . __dict__ ) config . num_classes = 8 # load the pretrained weight, with a different number of classes config . build () . load_state_dict ( \"/somewhere/my_model_backbone_a_x.pth\" ) If you have any issues, feel free to open one on GitHub","title":"Configurations"},{"location":"tutorial/configurations/#basic-config","text":"Let's see how to create a basic config. First, we need a model from torch import nn class MyModel ( nn . Module ): def __init__ ( in_channels : int , out_channels : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) def forward ( self , x ): return self . conv ( x ) Then we can create it's configuration from glasses.config import Config # Let's create it's configuration @dataclass class MyConfig ( Config ): in_channels : int out_channels : int def build ( self ) -> nn . Module : # create a `MyModel` instance using `MyConfig` return MyModel ( ** self . __dict__ ) We can now invoke the build method, which will create the model model : MyModel = MyConfig ( 2 , 2 ) . build () # same as model : MyModel = MyModel ( 2 , 2 ) Nothing very special.","title":"Basic Config"},{"location":"tutorial/configurations/#nested-config","text":"Let's see how to create a nested config . Assume we have a model that takes a backbone and has a fixed head. from torch import nn class MyModel ( nn . Module ): def __init__ ( self , backbone : nn . Module , channels : int , num_classes : int ): super () . __init__ () self . backbone = backbone self . head = nn . Conv2d ( channels , num_classes , kernel_size = 1 ) def forward ( self , x ): features = self . backbone ( x ) out = self . head ( features [ - 1 ]) # use last feature return out Our config will be nested since backbone has its own configuration. from glasses.config import Config @dataclass class MyConfig ( Config ): backbone_config : Config channels : int num_classes : int def build ( self ) -> nn . Module : backbone = backbone_config . build () # create a `MyModel` instance using `MyConfig` return MyModel ( backbone , self . channels , self . num_classes ) As expected, we must have configs for the different backbones we want to use. from torch import nn from glasses.config import Config class BackboneA ( nn . Module ): def __init__ ( ... ): ... @dataclass class BackboneAConfig ( Config ): ... class BackboneB ( nn . Module ): def __init__ ( ... ): .... @dataclass class BackboneBConfig ( Config ): ... Then, we can pass any backbone to MyConfig . config = MyConfig ( backbone_config = BackboneAConfig ( ... ), ... ) config . build () # build model with backbone A config = MyConfig ( backbone_config = BackboneBConfig ( ... ), ... ) config . build () # build model with backbone B The main advantage of the config system is when we need to save a specific model version. For instance, assume I have trained MyModel with BackboneA on dataset X . Its config will look like this: my_model_backbone_a_x = MyConfig ( backbone_config = BackboneAConfig ( ... ), channels = 64 , num_classes = 10 ) Therefore, at any point in time, I can recreate the model and load its pretrained weights. my_model_backbone_a_x . build () . load_state_dict ( \"/somewhere/my_model_backbone_a_x.pth\" ) Now, what if I want to use my_model_backbone_a_x architecture but just change a small part? Maybe the number of classes? # clone the config config = MyConfig ( ** my_model_backbone_a_x . __dict__ ) config . num_classes = 8 # load the pretrained weight, with a different number of classes config . build () . load_state_dict ( \"/somewhere/my_model_backbone_a_x.pth\" ) If you have any issues, feel free to open one on GitHub","title":"Nested Config"},{"location":"tutorial/tests/","text":"Testing your code is crucial to ensure its correctness. We try to help you smooth this process by proving an easy-to-use recipe. Let's see how to add a test for a model to glasses. Setup Let's take an example, you have created a new image classification model called BoringModel . Following our structure, its config should be in glasses/models/vision/image/classification/boring_model/config.py \u251c\u2500\u2500 glasses \u2502 \u251c\u2500\u2500 models \u2502 \u2502 \u2514\u2500\u2500 vision \u2502 \u2502 \u251c\u2500\u2500 image \u2502 \u2502 \u2502 \u251c\u2500\u2500 classification \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 boring_model \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 config.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 model.py Its config may look like # config.py from dataclasses import dataclass from glasses.config import Config from .model import BoringModelForImageClassification @dataclass class BoringModelForImageClassificationConfig ( Config ): in_channels : int = 3 hidden_size : int = 32 num_classes : int = 10 def build ( self ) -> BoringModelForImageClassification : return BoringModelForImageClassification ( ** self . __dict__ ) and the model # model.py from torch import Tensor , nn from ..base import ( ModelForImageClassification , ModelForImageClassificationOutput ) class BoringModelForImageClassification ( ModelForImageClassification ): def __init__ ( self , in_channels : int , hidden_size : int , num_classes : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , hidden_size , kernel_size = 3 ) self . avg = nn . AdaptiveAvgPool2d ( 1 ) self . fc = nn . Linear ( hidden_size , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : x = self . conv ( pixel_values ) x = self . avg ( x ) . flatten ( 1 ) logits = self . fc ( x ) return ModelForImageClassificationOutput ( logits = logits ) Cool, let's see how to test our BoringModelForImageClassification Test it! To mimic our internal structure, we will add a test at glasses/models/vision/image/classification/boring_model/test_boring_model.py . If you have used our cli you should have it by default. Since we know which config the model needs and we fixed the output, test it it's straightforward. We provide you a handy function, model_tester . You need to pass to it your config, the input dictionary, the model type and expected outputs shape and optionally expected outputs values. So, first create a new config for testing the model # test_boring_model.py import torch from glasses.models.vision.image.classification.boring_model import BoringModelForImageClassificationConfig , BoringModelForImageClassification from glasses.models.vision.image.classification import \\ ModelForImageClassificationOutput from tests.model_tester import model_tester def test_boring_model (): batch_size = 2 # create a config for a small model, we need our test to be fast config = BoringModelForImageClassificationConfig ( in_channels = 3 , hidden_size = 16 , num_classes = 10 ) Then, create an input_dict , this will be passed to your model. *8It must match the forward arguments of your model` Then, the shape of the expected output: # test_boring_model.py def test_boring_model (): #... output_shape_dict = { \"logits\" : ( batch_size , config . num_classes )} Finally, we can call model_tester # test_boring_model.py def test_boring_model (): #... model_tester ( config , input_dict , output_shape_dict , ModelForImageClassificationOutput ) All together: # test_boring_model.py import torch from glasses.models.vision.image.classification.boring_model import BoringModelForImageClassificationConfig , BoringModelForImageClassification from glasses.models.vision.image.classification import \\ ModelForImageClassificationOutput from tests.model_tester import model_tester def test_boring_model (): batch_size = 2 # create a config for a small model, we need our test to be fast config = BoringModelForImageClassificationConfig ( in_channels = 3 , hidden_size = 16 , num_classes = 10 ) # create the input dict, something like input_dict = { \"pixel_values\" : torch . randn ( ( batch_size , config . in_channels , 56 , 56 ) ) } output_shape_dict = { \"logits\" : ( batch_size , config . num_classes )} model_tester ( config , input_dict , output_shape_dict , ModelForImageClassificationOutput ) We can now run the tests with pytest $ python -m pytest ./tests/models/vision/image/classification/boring_model =========================================================================================================== test session starts =========================================================================================================== platform linux -- Python 3.9.12, pytest-7.1.3, pluggy-1.0.0 rootdir: /home/zuppif/Documents/glasses-2.0 collected 1 item tests/models/vision/image/classification/boring_model/test_boring_model.py . [100%] ============================================================================================================ 1 passed in 0.05s ============================================================================================================ Done it! \ud83e\uddea\ud83e\uddea\ud83e\uddea","title":"Tests"},{"location":"tutorial/tests/#setup","text":"Let's take an example, you have created a new image classification model called BoringModel . Following our structure, its config should be in glasses/models/vision/image/classification/boring_model/config.py \u251c\u2500\u2500 glasses \u2502 \u251c\u2500\u2500 models \u2502 \u2502 \u2514\u2500\u2500 vision \u2502 \u2502 \u251c\u2500\u2500 image \u2502 \u2502 \u2502 \u251c\u2500\u2500 classification \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 boring_model \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 config.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 model.py Its config may look like # config.py from dataclasses import dataclass from glasses.config import Config from .model import BoringModelForImageClassification @dataclass class BoringModelForImageClassificationConfig ( Config ): in_channels : int = 3 hidden_size : int = 32 num_classes : int = 10 def build ( self ) -> BoringModelForImageClassification : return BoringModelForImageClassification ( ** self . __dict__ ) and the model # model.py from torch import Tensor , nn from ..base import ( ModelForImageClassification , ModelForImageClassificationOutput ) class BoringModelForImageClassification ( ModelForImageClassification ): def __init__ ( self , in_channels : int , hidden_size : int , num_classes : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , hidden_size , kernel_size = 3 ) self . avg = nn . AdaptiveAvgPool2d ( 1 ) self . fc = nn . Linear ( hidden_size , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForImageClassificationOutput : x = self . conv ( pixel_values ) x = self . avg ( x ) . flatten ( 1 ) logits = self . fc ( x ) return ModelForImageClassificationOutput ( logits = logits ) Cool, let's see how to test our BoringModelForImageClassification","title":"Setup"},{"location":"tutorial/tests/#test-it","text":"To mimic our internal structure, we will add a test at glasses/models/vision/image/classification/boring_model/test_boring_model.py . If you have used our cli you should have it by default. Since we know which config the model needs and we fixed the output, test it it's straightforward. We provide you a handy function, model_tester . You need to pass to it your config, the input dictionary, the model type and expected outputs shape and optionally expected outputs values. So, first create a new config for testing the model # test_boring_model.py import torch from glasses.models.vision.image.classification.boring_model import BoringModelForImageClassificationConfig , BoringModelForImageClassification from glasses.models.vision.image.classification import \\ ModelForImageClassificationOutput from tests.model_tester import model_tester def test_boring_model (): batch_size = 2 # create a config for a small model, we need our test to be fast config = BoringModelForImageClassificationConfig ( in_channels = 3 , hidden_size = 16 , num_classes = 10 ) Then, create an input_dict , this will be passed to your model. *8It must match the forward arguments of your model` Then, the shape of the expected output: # test_boring_model.py def test_boring_model (): #... output_shape_dict = { \"logits\" : ( batch_size , config . num_classes )} Finally, we can call model_tester # test_boring_model.py def test_boring_model (): #... model_tester ( config , input_dict , output_shape_dict , ModelForImageClassificationOutput ) All together: # test_boring_model.py import torch from glasses.models.vision.image.classification.boring_model import BoringModelForImageClassificationConfig , BoringModelForImageClassification from glasses.models.vision.image.classification import \\ ModelForImageClassificationOutput from tests.model_tester import model_tester def test_boring_model (): batch_size = 2 # create a config for a small model, we need our test to be fast config = BoringModelForImageClassificationConfig ( in_channels = 3 , hidden_size = 16 , num_classes = 10 ) # create the input dict, something like input_dict = { \"pixel_values\" : torch . randn ( ( batch_size , config . in_channels , 56 , 56 ) ) } output_shape_dict = { \"logits\" : ( batch_size , config . num_classes )} model_tester ( config , input_dict , output_shape_dict , ModelForImageClassificationOutput ) We can now run the tests with pytest $ python -m pytest ./tests/models/vision/image/classification/boring_model =========================================================================================================== test session starts =========================================================================================================== platform linux -- Python 3.9.12, pytest-7.1.3, pluggy-1.0.0 rootdir: /home/zuppif/Documents/glasses-2.0 collected 1 item tests/models/vision/image/classification/boring_model/test_boring_model.py . [100%] ============================================================================================================ 1 passed in 0.05s ============================================================================================================ Done it! \ud83e\uddea\ud83e\uddea\ud83e\uddea","title":"Test it!"}]}