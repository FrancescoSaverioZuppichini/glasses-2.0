{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A Modern Doc Template for your Python Project \ud83d\udc85 This website was build with: mkdocs.org mkdocs-material pymdown Features out of the box amazing good looking website thanks to mkdocs.org and mkdocs-material a wide array of cool features, such as code highlight, admonition, latex rendering and more thanks to pymdown FastApi cool terminal rendering auto documentation for code thanks to mkdocstrings automatic build thanks to github action, see the CI/CD page Getting Started Head over the getting started guide Cool stuff Code Highlights See here for doc print ( \"Hello World!\" ) Admonition See here for doc Hello There General Kenobi! Hello There General Kenobi! Error Boom! Latex See here for doc Inline \\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\) , \\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\) . Block \\[ E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i,j}w_{ij}v_i h_j - \\sum_i b_i v_i - \\sum_j c_j h_j \\] \\[\\begin{align} p(v_i=1|\\mathbf{h}) & = \\sigma\\left(\\sum_j w_{ij}h_j + b_i\\right) \\\\ p(h_j=1|\\mathbf{v}) & = \\sigma\\left(\\sum_i w_{ij}v_i + c_j\\right) \\end{align}\\]","title":"Docs"},{"location":"#a-modern-doc-template-for-your-python-project","text":"This website was build with: mkdocs.org mkdocs-material pymdown","title":"A Modern Doc Template for your Python Project \ud83d\udc85"},{"location":"#features","text":"out of the box amazing good looking website thanks to mkdocs.org and mkdocs-material a wide array of cool features, such as code highlight, admonition, latex rendering and more thanks to pymdown FastApi cool terminal rendering auto documentation for code thanks to mkdocstrings automatic build thanks to github action, see the CI/CD page","title":"Features"},{"location":"#getting-started","text":"Head over the getting started guide","title":"Getting Started"},{"location":"#cool-stuff","text":"","title":"Cool stuff"},{"location":"#code-highlights","text":"See here for doc print ( \"Hello World!\" )","title":"Code Highlights"},{"location":"#admonition","text":"See here for doc Hello There General Kenobi! Hello There General Kenobi! Error Boom!","title":"Admonition"},{"location":"#latex","text":"See here for doc Inline \\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\) , \\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\) . Block \\[ E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i,j}w_{ij}v_i h_j - \\sum_i b_i v_i - \\sum_j c_j h_j \\] \\[\\begin{align} p(v_i=1|\\mathbf{h}) & = \\sigma\\left(\\sum_j w_{ij}h_j + b_i\\right) \\\\ p(h_j=1|\\mathbf{v}) & = \\sigma\\left(\\sum_i w_{ij}v_i + c_j\\right) \\end{align}\\]","title":"Latex"},{"location":"contributing/","text":"Naming Convention Names Keep the variables names short but meaningfull - convolution -> conv - activation -> act - linear/dense -> fc - batchnorm -> bn .. etc Design We follow the single responsability principle where each class/function does only one thing. Here we are subclassing nn.Sequential for simplicity. class MyModel ( nn . Module ): def __init__ ( ... ): super () . __init__ () self . embedder = nn . Sequential ( nn . Conv2d ( .... ), nn . BatchNorm2d ( ... ), nn . ReLU () ) self . encoder = nn . Sequential ( nn . Conv2d ( ... ), nn . Conv2d ( ... ), nn . Conv2d ( ... ) ) self . head = nn . Sequential ( nn . Linear ( ... ) ) class MyModelEmbedder ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Conv2d ( ... ), nn . BatchNorm2d ( ... ), nn . ReLU () ) class MyModelEncoder ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Conv2d ( ... ), nn . Conv2d ( ... ), nn . Conv2d ( ... ) ) class MyModelHead ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Linear ( ... )) class MyModel ( nn . Module ): def __init__ ( ... ): super () . __init__ () self . embedder = MyModelEmbedder ( ... ) self . encoder = MyModelEncoder ( ... ) self . head = MyModelHead ( ... )","title":"Contributing"},{"location":"contributing/#naming-convention","text":"","title":"Naming Convention"},{"location":"contributing/#names","text":"Keep the variables names short but meaningfull - convolution -> conv - activation -> act - linear/dense -> fc - batchnorm -> bn .. etc","title":"Names"},{"location":"contributing/#design","text":"We follow the single responsability principle where each class/function does only one thing. Here we are subclassing nn.Sequential for simplicity. class MyModel ( nn . Module ): def __init__ ( ... ): super () . __init__ () self . embedder = nn . Sequential ( nn . Conv2d ( .... ), nn . BatchNorm2d ( ... ), nn . ReLU () ) self . encoder = nn . Sequential ( nn . Conv2d ( ... ), nn . Conv2d ( ... ), nn . Conv2d ( ... ) ) self . head = nn . Sequential ( nn . Linear ( ... ) ) class MyModelEmbedder ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Conv2d ( ... ), nn . BatchNorm2d ( ... ), nn . ReLU () ) class MyModelEncoder ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Conv2d ( ... ), nn . Conv2d ( ... ), nn . Conv2d ( ... ) ) class MyModelHead ( nn . Sequential ): def __init__ ( ... ): super () . __init__ ( nn . Linear ( ... )) class MyModel ( nn . Module ): def __init__ ( ... ): super () . __init__ () self . embedder = MyModelEmbedder ( ... ) self . encoder = MyModelEncoder ( ... ) self . head = MyModelHead ( ... )","title":"Design"},{"location":"getting_started/","text":"$ AutoModel () . from_pretrained ( \"my_name\" ) [ 06 / 21 / 22 18 : 13 : 40 ] WARNING Error ( s ) in loading state_dict for AnyModelForClassification : base . py : 54 Unexpected key ( s ) in state_dict : \"head.fc.weight\" , \"head.fc.bias\" . INFO Loaded pretrained weights for dummy - d0 .","title":"Getting Started"},{"location":"models/vision/classification/dummy/","text":"Dummy model!","title":"Dummy"},{"location":"reference/SUMMARY/","text":"glasses config logger models auto base model_zoo utils transform transform vision auto backbones base vit config model zoo classification base common config model heads base linear_head config model outputs vit config zoo necks Neck storage base local tests auto package a model zoo b model zoo test_auto test_auto_utils conftest fixtures types","title":"SUMMARY"},{"location":"reference/logger/","text":"logger = logging . getLogger ( 'glasses' ) module-attribute","title":"logger"},{"location":"reference/logger/#glasses.logger.logger","text":"","title":"logger"},{"location":"reference/types/","text":"StateDict = Dict [ str , Tensor ] module-attribute","title":"types"},{"location":"reference/types/#glasses.types.StateDict","text":"","title":"StateDict"},{"location":"reference/config/","text":"Config dataclass Base class for configurations, all configuration must inherit from this class. For a in depth tutorial about configs, head over Configurations Important Models are not coupled with Config , therefore they are unaware of the configuration system. Each Config is linked to a specific model, not viceversa. Note A Config holds what is needed to create a model. Therefore, they are perfect to share custom version of a specific architecture. A Config is data container . Thus, it must not have any side effect , any logic that requires the Config values to be somehow processed must be implemented in the Config.build function. A custom configuration can be written as follows: from glasses.config import Config # Assume we have a model class MyModel ( nn . Module ): def __init__ ( in_channels : int , out_channels : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) def forward ( self , x ): return self . conv ( x ) # Let's create it's configuration @dataclass class MyConfig ( Config ): in_channels : int out_channels : int def build ( self ) -> nn . Module : # create a `MyModel` instance using `MyConfig` return MyModel ( ** self . __dict__ ) model : MyModel = MyConfig ( 2 , 2 ) . build () Each Config is linked to a specific model, not viceversa. Models had no idea about the configuration system and can be created normally as you expect with their constructor. Nested Configurations Source code in glasses/config/__init__.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @dataclass class Config : \"\"\" Base class for configurations, all configuration **must** inherit from this class. For a in depth tutorial about configs, head over [Configurations](/) !!! important Models are not coupled with `Config`, therefore they are unaware of the configuration system. Each `Config` is linked to a specific model, not viceversa. !!! note A `Config` holds what is needed to create a model. Therefore, they are perfect to share custom version of a specific architecture. A `Config` is **data container**. Thus, it **must not have any side effect**, any logic that requires the `Config` values to be somehow processed must be implemented in the [`Config.build`](#glasses.config.Config.build) function. A custom configuration can be written as follows: ```python from glasses.config import Config # Assume we have a model class MyModel(nn.Module): def __init__(in_channels: int, out_channels: int): super().__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) def forward(self, x): return self.conv(x) # Let's create it's configuration @dataclass class MyConfig(Config): in_channels: int out_channels: int def build(self) -> nn.Module: # create a `MyModel` instance using `MyConfig` return MyModel(**self.__dict__) model: MyModel = MyConfig(2, 2).build() ``` Each Config is linked to a specific model, not viceversa. Models had no idea about the configuration system and can be created normally as you expect with their constructor. ## Nested Configurations \"\"\" def build ( self ) -> nn . Module : raise NotImplemented build () Source code in glasses/config/__init__.py 53 54 def build ( self ) -> nn . Module : raise NotImplemented","title":"config"},{"location":"reference/config/#glasses.config.Config","text":"Base class for configurations, all configuration must inherit from this class. For a in depth tutorial about configs, head over Configurations Important Models are not coupled with Config , therefore they are unaware of the configuration system. Each Config is linked to a specific model, not viceversa. Note A Config holds what is needed to create a model. Therefore, they are perfect to share custom version of a specific architecture. A Config is data container . Thus, it must not have any side effect , any logic that requires the Config values to be somehow processed must be implemented in the Config.build function. A custom configuration can be written as follows: from glasses.config import Config # Assume we have a model class MyModel ( nn . Module ): def __init__ ( in_channels : int , out_channels : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) def forward ( self , x ): return self . conv ( x ) # Let's create it's configuration @dataclass class MyConfig ( Config ): in_channels : int out_channels : int def build ( self ) -> nn . Module : # create a `MyModel` instance using `MyConfig` return MyModel ( ** self . __dict__ ) model : MyModel = MyConfig ( 2 , 2 ) . build () Each Config is linked to a specific model, not viceversa. Models had no idea about the configuration system and can be created normally as you expect with their constructor.","title":"Config"},{"location":"reference/config/#glasses.config.Config--nested-configurations","text":"Source code in glasses/config/__init__.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @dataclass class Config : \"\"\" Base class for configurations, all configuration **must** inherit from this class. For a in depth tutorial about configs, head over [Configurations](/) !!! important Models are not coupled with `Config`, therefore they are unaware of the configuration system. Each `Config` is linked to a specific model, not viceversa. !!! note A `Config` holds what is needed to create a model. Therefore, they are perfect to share custom version of a specific architecture. A `Config` is **data container**. Thus, it **must not have any side effect**, any logic that requires the `Config` values to be somehow processed must be implemented in the [`Config.build`](#glasses.config.Config.build) function. A custom configuration can be written as follows: ```python from glasses.config import Config # Assume we have a model class MyModel(nn.Module): def __init__(in_channels: int, out_channels: int): super().__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) def forward(self, x): return self.conv(x) # Let's create it's configuration @dataclass class MyConfig(Config): in_channels: int out_channels: int def build(self) -> nn.Module: # create a `MyModel` instance using `MyConfig` return MyModel(**self.__dict__) model: MyModel = MyConfig(2, 2).build() ``` Each Config is linked to a specific model, not viceversa. Models had no idea about the configuration system and can be created normally as you expect with their constructor. ## Nested Configurations \"\"\" def build ( self ) -> nn . Module : raise NotImplemented","title":"Nested Configurations"},{"location":"reference/config/#glasses.config.Config.build","text":"Source code in glasses/config/__init__.py 53 54 def build ( self ) -> nn . Module : raise NotImplemented","title":"build()"},{"location":"reference/models/","text":"","title":"Index"},{"location":"reference/models/auto/","text":"","title":"Index"},{"location":"reference/models/auto/base/","text":"AutoModel The base AutoModel class. Usage: auto_model = AutoModel () model = auto_model . from_name ( \"my_name\" ) model = auto_model . from_pretrained ( \"my_name\" ) model = auto_model . from_pretrained ( \"my_name\" , my_config ) Source code in glasses/models/auto/base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class AutoModel : \"\"\"The base `AutoModel` class. Usage: ```python auto_model = AutoModel() model = auto_model.from_name(\"my_name\") model = auto_model.from_pretrained(\"my_name\") model = auto_model.from_pretrained(\"my_name\", my_config) ``` \"\"\" names_to_configs : Dict [ str , Callable [[], Config ]] \"\"\"Holds the map from name to config type\"\"\" @classmethod def get_config_from_name ( cls , name : str ) -> Config : return cls . names_to_configs [ name ]() @classmethod def from_name ( cls , name : str ): if name not in cls . names_to_configs : suggestions = difflib . get_close_matches ( name , cls . names_to_configs . keys ()) msg = f 'Model \" { name } \" does not exists.' if len ( suggestions ) > 0 : msg += f ' Did you mean \" { suggestions [ 0 ] } ?\"' raise KeyError ( msg ) config = cls . names_to_configs [ name ]() return config . build () @classmethod def from_pretrained ( cls , name : str , config : Optional [ Config ] = None , storage : Storage = None ) -> nn . Module : storage = LocalStorage () if storage is None else storage model = cls . from_name ( name ) if config is None else config . build () state_dict , _ = storage . get ( name ) try : model . load_state_dict ( state_dict ) except RuntimeError as e : logger . warning ( str ( e )) logger . info ( f \"Loaded pretrained weights for { name } .\" ) return model , config names_to_configs : Dict [ str , Callable [[], Config ]] class-attribute Holds the map from name to config type from_name ( name ) classmethod Source code in glasses/models/auto/base.py 32 33 34 35 36 37 38 39 40 41 42 @classmethod def from_name ( cls , name : str ): if name not in cls . names_to_configs : suggestions = difflib . get_close_matches ( name , cls . names_to_configs . keys ()) msg = f 'Model \" { name } \" does not exists.' if len ( suggestions ) > 0 : msg += f ' Did you mean \" { suggestions [ 0 ] } ?\"' raise KeyError ( msg ) config = cls . names_to_configs [ name ]() return config . build () from_pretrained ( name , config = None , storage = None ) classmethod Source code in glasses/models/auto/base.py 44 45 46 47 48 49 50 51 52 53 54 55 56 @classmethod def from_pretrained ( cls , name : str , config : Optional [ Config ] = None , storage : Storage = None ) -> nn . Module : storage = LocalStorage () if storage is None else storage model = cls . from_name ( name ) if config is None else config . build () state_dict , _ = storage . get ( name ) try : model . load_state_dict ( state_dict ) except RuntimeError as e : logger . warning ( str ( e )) logger . info ( f \"Loaded pretrained weights for { name } .\" ) return model , config get_config_from_name ( name ) classmethod Source code in glasses/models/auto/base.py 28 29 30 @classmethod def get_config_from_name ( cls , name : str ) -> Config : return cls . names_to_configs [ name ]()","title":"base"},{"location":"reference/models/auto/base/#glasses.models.auto.base.AutoModel","text":"The base AutoModel class. Usage: auto_model = AutoModel () model = auto_model . from_name ( \"my_name\" ) model = auto_model . from_pretrained ( \"my_name\" ) model = auto_model . from_pretrained ( \"my_name\" , my_config ) Source code in glasses/models/auto/base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class AutoModel : \"\"\"The base `AutoModel` class. Usage: ```python auto_model = AutoModel() model = auto_model.from_name(\"my_name\") model = auto_model.from_pretrained(\"my_name\") model = auto_model.from_pretrained(\"my_name\", my_config) ``` \"\"\" names_to_configs : Dict [ str , Callable [[], Config ]] \"\"\"Holds the map from name to config type\"\"\" @classmethod def get_config_from_name ( cls , name : str ) -> Config : return cls . names_to_configs [ name ]() @classmethod def from_name ( cls , name : str ): if name not in cls . names_to_configs : suggestions = difflib . get_close_matches ( name , cls . names_to_configs . keys ()) msg = f 'Model \" { name } \" does not exists.' if len ( suggestions ) > 0 : msg += f ' Did you mean \" { suggestions [ 0 ] } ?\"' raise KeyError ( msg ) config = cls . names_to_configs [ name ]() return config . build () @classmethod def from_pretrained ( cls , name : str , config : Optional [ Config ] = None , storage : Storage = None ) -> nn . Module : storage = LocalStorage () if storage is None else storage model = cls . from_name ( name ) if config is None else config . build () state_dict , _ = storage . get ( name ) try : model . load_state_dict ( state_dict ) except RuntimeError as e : logger . warning ( str ( e )) logger . info ( f \"Loaded pretrained weights for { name } .\" ) return model , config","title":"AutoModel"},{"location":"reference/models/auto/base/#glasses.models.auto.base.AutoModel.names_to_configs","text":"Holds the map from name to config type","title":"names_to_configs"},{"location":"reference/models/auto/base/#glasses.models.auto.base.AutoModel.from_name","text":"Source code in glasses/models/auto/base.py 32 33 34 35 36 37 38 39 40 41 42 @classmethod def from_name ( cls , name : str ): if name not in cls . names_to_configs : suggestions = difflib . get_close_matches ( name , cls . names_to_configs . keys ()) msg = f 'Model \" { name } \" does not exists.' if len ( suggestions ) > 0 : msg += f ' Did you mean \" { suggestions [ 0 ] } ?\"' raise KeyError ( msg ) config = cls . names_to_configs [ name ]() return config . build ()","title":"from_name()"},{"location":"reference/models/auto/base/#glasses.models.auto.base.AutoModel.from_pretrained","text":"Source code in glasses/models/auto/base.py 44 45 46 47 48 49 50 51 52 53 54 55 56 @classmethod def from_pretrained ( cls , name : str , config : Optional [ Config ] = None , storage : Storage = None ) -> nn . Module : storage = LocalStorage () if storage is None else storage model = cls . from_name ( name ) if config is None else config . build () state_dict , _ = storage . get ( name ) try : model . load_state_dict ( state_dict ) except RuntimeError as e : logger . warning ( str ( e )) logger . info ( f \"Loaded pretrained weights for { name } .\" ) return model , config","title":"from_pretrained()"},{"location":"reference/models/auto/base/#glasses.models.auto.base.AutoModel.get_config_from_name","text":"Source code in glasses/models/auto/base.py 28 29 30 @classmethod def get_config_from_name ( cls , name : str ) -> Config : return cls . names_to_configs [ name ]()","title":"get_config_from_name()"},{"location":"reference/models/auto/model_zoo/","text":"ModelZoo Bases: dict Source code in glasses/models/auto/model_zoo.py 1 2 class ModelZoo ( dict ): pass","title":"model_zoo"},{"location":"reference/models/auto/model_zoo/#glasses.models.auto.model_zoo.ModelZoo","text":"Bases: dict Source code in glasses/models/auto/model_zoo.py 1 2 class ModelZoo ( dict ): pass","title":"ModelZoo"},{"location":"reference/models/auto/utils/","text":"get_names_to_configs_map ( * args , ** kwargs ) Source code in glasses/models/auto/utils.py 28 29 30 31 32 33 34 35 36 37 38 39 def get_names_to_configs_map ( * args , ** kwargs ): names_to_models_map = {} for module in iter_models_modules ( * args , ** kwargs ): submodule = importlib . import_module ( \".\" , f \" { module } .zoo\" ) try : zoo = vars ( submodule )[ \"zoo\" ] except KeyError : raise KeyError ( f \"A `zoo.py` was found in { module } but no `zoo` was defined.\" ) names_to_models_map = { ** names_to_models_map , ** zoo } return names_to_models_map iter_models_modules ( package , ignore_dirs = None ) Source code in glasses/models/auto/utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def iter_models_modules ( package : str , ignore_dirs : Optional [ List [ str ]] = None ) -> Iterator [ str ]: ignore_dirs = [] if ignore_dirs is None else ignore_dirs # the following folders will be skipeed by default ignore_dirs += [ \"auto\" , \"heads\" , \"__pycache__\" , \"common\" ] # we import the package module = importlib . import_module ( \".\" , package ) if not module . __file__ : raise ModuleNotFoundError ( f \" { package } doesn't exist.\" ) # and we get the path to the folder it's contained module_path = Path ( module . __file__ ) . parent # then we iterate all the subdirs and we look for packages to import for file_or_dir in module_path . iterdir (): # if we have found a dir and it's not in ignore is_valid_dir = file_or_dir . is_dir () and file_or_dir . stem not in ignore_dirs if is_valid_dir : has_a_config = ( file_or_dir / \"config.py\" ) . exists () if has_a_config : yield f \" { package } . { file_or_dir . stem } \"","title":"utils"},{"location":"reference/models/auto/utils/#glasses.models.auto.utils.get_names_to_configs_map","text":"Source code in glasses/models/auto/utils.py 28 29 30 31 32 33 34 35 36 37 38 39 def get_names_to_configs_map ( * args , ** kwargs ): names_to_models_map = {} for module in iter_models_modules ( * args , ** kwargs ): submodule = importlib . import_module ( \".\" , f \" { module } .zoo\" ) try : zoo = vars ( submodule )[ \"zoo\" ] except KeyError : raise KeyError ( f \"A `zoo.py` was found in { module } but no `zoo` was defined.\" ) names_to_models_map = { ** names_to_models_map , ** zoo } return names_to_models_map","title":"get_names_to_configs_map()"},{"location":"reference/models/auto/utils/#glasses.models.auto.utils.iter_models_modules","text":"Source code in glasses/models/auto/utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def iter_models_modules ( package : str , ignore_dirs : Optional [ List [ str ]] = None ) -> Iterator [ str ]: ignore_dirs = [] if ignore_dirs is None else ignore_dirs # the following folders will be skipeed by default ignore_dirs += [ \"auto\" , \"heads\" , \"__pycache__\" , \"common\" ] # we import the package module = importlib . import_module ( \".\" , package ) if not module . __file__ : raise ModuleNotFoundError ( f \" { package } doesn't exist.\" ) # and we get the path to the folder it's contained module_path = Path ( module . __file__ ) . parent # then we iterate all the subdirs and we look for packages to import for file_or_dir in module_path . iterdir (): # if we have found a dir and it's not in ignore is_valid_dir = file_or_dir . is_dir () and file_or_dir . stem not in ignore_dirs if is_valid_dir : has_a_config = ( file_or_dir / \"config.py\" ) . exists () if has_a_config : yield f \" { package } . { file_or_dir . stem } \"","title":"iter_models_modules()"},{"location":"reference/models/transform/","text":"","title":"Index"},{"location":"reference/models/transform/transform/","text":"ApplyToKeys dataclass Source code in glasses/models/transform/transform.py 5 6 7 8 9 10 11 12 13 14 @dataclass class ApplyToKeys : transform : Callable keys : List [ str ] def __call__ ( self , data : Dict [ str , Any ]) -> Dict [ str , Any ]: for key , val in data . items (): if key in self . keys : data [ key ] = self . transform ( val ) return data keys : List [ str ] class-attribute transform : Callable class-attribute __call__ ( data ) Source code in glasses/models/transform/transform.py 10 11 12 13 14 def __call__ ( self , data : Dict [ str , Any ]) -> Dict [ str , Any ]: for key , val in data . items (): if key in self . keys : data [ key ] = self . transform ( val ) return data","title":"transform"},{"location":"reference/models/transform/transform/#glasses.models.transform.transform.ApplyToKeys","text":"Source code in glasses/models/transform/transform.py 5 6 7 8 9 10 11 12 13 14 @dataclass class ApplyToKeys : transform : Callable keys : List [ str ] def __call__ ( self , data : Dict [ str , Any ]) -> Dict [ str , Any ]: for key , val in data . items (): if key in self . keys : data [ key ] = self . transform ( val ) return data","title":"ApplyToKeys"},{"location":"reference/models/transform/transform/#glasses.models.transform.transform.ApplyToKeys.keys","text":"","title":"keys"},{"location":"reference/models/transform/transform/#glasses.models.transform.transform.ApplyToKeys.transform","text":"","title":"transform"},{"location":"reference/models/transform/transform/#glasses.models.transform.transform.ApplyToKeys.__call__","text":"Source code in glasses/models/transform/transform.py 10 11 12 13 14 def __call__ ( self , data : Dict [ str , Any ]) -> Dict [ str , Any ]: for key , val in data . items (): if key in self . keys : data [ key ] = self . transform ( val ) return data","title":"__call__()"},{"location":"reference/models/vision/","text":"","title":"Index"},{"location":"reference/models/vision/auto/","text":"AutoModelBackbone Bases: AutoModel Source code in glasses/models/vision/auto.py 5 6 class AutoModelBackbone ( AutoModel ): names_to_configs = get_names_to_configs_map ( \"glasses.models.vision.backbones\" ) names_to_configs = get_names_to_configs_map ( 'glasses.models.vision.backbones' ) class-attribute AutoModelForClassification Bases: AutoModel Source code in glasses/models/vision/auto.py 9 10 class AutoModelForClassification ( AutoModel ): names_to_configs = get_names_to_configs_map ( \"glasses.models.vision.classification\" ) names_to_configs = get_names_to_configs_map ( 'glasses.models.vision.classification' ) class-attribute","title":"auto"},{"location":"reference/models/vision/auto/#glasses.models.vision.auto.AutoModelBackbone","text":"Bases: AutoModel Source code in glasses/models/vision/auto.py 5 6 class AutoModelBackbone ( AutoModel ): names_to_configs = get_names_to_configs_map ( \"glasses.models.vision.backbones\" )","title":"AutoModelBackbone"},{"location":"reference/models/vision/auto/#glasses.models.vision.auto.AutoModelBackbone.names_to_configs","text":"","title":"names_to_configs"},{"location":"reference/models/vision/auto/#glasses.models.vision.auto.AutoModelForClassification","text":"Bases: AutoModel Source code in glasses/models/vision/auto.py 9 10 class AutoModelForClassification ( AutoModel ): names_to_configs = get_names_to_configs_map ( \"glasses.models.vision.classification\" )","title":"AutoModelForClassification"},{"location":"reference/models/vision/auto/#glasses.models.vision.auto.AutoModelForClassification.names_to_configs","text":"","title":"names_to_configs"},{"location":"reference/models/vision/backbones/","text":"","title":"Index"},{"location":"reference/models/vision/backbones/base/","text":"Backbone Bases: nn . Module Source code in glasses/models/vision/backbones/base.py 6 7 8 class Backbone ( nn . Module ): def forward ( self , pixel_values : Tensor ) -> List [ Tensor ]: raise NotImplemented forward ( pixel_values ) Source code in glasses/models/vision/backbones/base.py 7 8 def forward ( self , pixel_values : Tensor ) -> List [ Tensor ]: raise NotImplemented","title":"base"},{"location":"reference/models/vision/backbones/base/#glasses.models.vision.backbones.base.Backbone","text":"Bases: nn . Module Source code in glasses/models/vision/backbones/base.py 6 7 8 class Backbone ( nn . Module ): def forward ( self , pixel_values : Tensor ) -> List [ Tensor ]: raise NotImplemented","title":"Backbone"},{"location":"reference/models/vision/backbones/base/#glasses.models.vision.backbones.base.Backbone.forward","text":"Source code in glasses/models/vision/backbones/base.py 7 8 def forward ( self , pixel_values : Tensor ) -> List [ Tensor ]: raise NotImplemented","title":"forward()"},{"location":"reference/models/vision/backbones/vit/","text":"","title":"Index"},{"location":"reference/models/vision/backbones/vit/config/","text":"ViTBackboneConfig dataclass Bases: Config Source code in glasses/models/vision/backbones/vit/config.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @dataclass class ViTBackboneConfig ( Config ): img_size : int = 224 in_channels : int = 3 patch_size : int = 16 depth : int = 12 embed_dim : int = 768 num_heads : int = 12 attn_drop_p : float = 0.0 projection_drop_p : float = 0.2 qkv_bias : bool = False forward_expansion : int = 4 forward_drop_p : float = 0.2 activation : nn . Module = nn . GELU def build ( self ): return ViTBackbone ( ** self . __dict__ ) def pprint ( self ): print ( ** self . __dict__ ) activation : nn . Module = nn . GELU class-attribute attn_drop_p : float = 0.0 class-attribute depth : int = 12 class-attribute embed_dim : int = 768 class-attribute forward_drop_p : float = 0.2 class-attribute forward_expansion : int = 4 class-attribute img_size : int = 224 class-attribute in_channels : int = 3 class-attribute num_heads : int = 12 class-attribute patch_size : int = 16 class-attribute projection_drop_p : float = 0.2 class-attribute qkv_bias : bool = False class-attribute build () Source code in glasses/models/vision/backbones/vit/config.py 21 22 def build ( self ): return ViTBackbone ( ** self . __dict__ ) pprint () Source code in glasses/models/vision/backbones/vit/config.py 24 25 def pprint ( self ): print ( ** self . __dict__ )","title":"config"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig","text":"Bases: Config Source code in glasses/models/vision/backbones/vit/config.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @dataclass class ViTBackboneConfig ( Config ): img_size : int = 224 in_channels : int = 3 patch_size : int = 16 depth : int = 12 embed_dim : int = 768 num_heads : int = 12 attn_drop_p : float = 0.0 projection_drop_p : float = 0.2 qkv_bias : bool = False forward_expansion : int = 4 forward_drop_p : float = 0.2 activation : nn . Module = nn . GELU def build ( self ): return ViTBackbone ( ** self . __dict__ ) def pprint ( self ): print ( ** self . __dict__ )","title":"ViTBackboneConfig"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.activation","text":"","title":"activation"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.attn_drop_p","text":"","title":"attn_drop_p"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.depth","text":"","title":"depth"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.embed_dim","text":"","title":"embed_dim"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.forward_drop_p","text":"","title":"forward_drop_p"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.forward_expansion","text":"","title":"forward_expansion"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.img_size","text":"","title":"img_size"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.in_channels","text":"","title":"in_channels"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.num_heads","text":"","title":"num_heads"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.patch_size","text":"","title":"patch_size"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.projection_drop_p","text":"","title":"projection_drop_p"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.qkv_bias","text":"","title":"qkv_bias"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.build","text":"Source code in glasses/models/vision/backbones/vit/config.py 21 22 def build ( self ): return ViTBackbone ( ** self . __dict__ )","title":"build()"},{"location":"reference/models/vision/backbones/vit/config/#glasses.models.vision.backbones.vit.config.ViTBackboneConfig.pprint","text":"Source code in glasses/models/vision/backbones/vit/config.py 24 25 def pprint ( self ): print ( ** self . __dict__ )","title":"pprint()"},{"location":"reference/models/vision/backbones/vit/model/","text":"ResidualAddition Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 112 113 114 115 116 117 118 119 120 class ResidualAddition ( nn . Module ): def __init__ ( self , fn ): super () . __init__ () self . fn = fn def forward ( self , x , ** kwargs ): out = self . fn ( x , ** kwargs ) x = x + out return x fn = fn instance-attribute __init__ ( fn ) Source code in glasses/models/vision/backbones/vit/model.py 113 114 115 def __init__ ( self , fn ): super () . __init__ () self . fn = fn forward ( x , ** kwargs ) Source code in glasses/models/vision/backbones/vit/model.py 117 118 119 120 def forward ( self , x , ** kwargs ): out = self . fn ( x , ** kwargs ) x = x + out return x ViTAttentionBlock Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class ViTAttentionBlock ( nn . Module ): def __init__ ( self , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , ): super () . __init__ () self . embed_dim = embed_dim self . num_heads = num_heads # fuse the queries, keys and values in one matrix self . qkv = nn . Linear ( embed_dim , embed_dim * 3 , bias = qkv_bias ) self . att_drop = nn . Dropout ( attn_drop_p ) self . projection = nn . Sequential ( nn . Linear ( embed_dim , embed_dim ), nn . Dropout ( projection_drop_p ) ) self . scaling = ( self . embed_dim // num_heads ) ** - 0.5 def forward ( self , x : torch . Tensor , mask : torch . Tensor = None ) -> torch . Tensor : # split keys, queries and values in num_heads qkv = rearrange ( self . qkv ( x ), \"b n (qkv h d) -> (qkv) b h n d\" , h = self . num_heads , qkv = 3 ) queries , keys , values = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] # dot product, Q V^T, here we don't transpose before, so this is why # the sum is made on the last index of K energy = torch . einsum ( \"bhij, bhkj -> bhik\" , queries , keys ) * self . scaling if mask is not None : fill_value = torch . finfo ( torch . float32 ) . min energy . mask_fill ( ~ mask , fill_value ) att = F . softmax ( energy , dim =- 1 ) att = self . att_drop ( att ) # dot product out = torch . einsum ( \"bhij, bhjk -> bhik \" , att , values ) out = rearrange ( out , \"b h n d -> b n (h d)\" ) out = self . projection ( out ) return out att_drop = nn . Dropout ( attn_drop_p ) instance-attribute embed_dim = embed_dim instance-attribute num_heads = num_heads instance-attribute projection = nn . Sequential ( nn . Linear ( embed_dim , embed_dim ), nn . Dropout ( projection_drop_p )) instance-attribute qkv = nn . Linear ( embed_dim , embed_dim * 3 , bias = qkv_bias ) instance-attribute scaling = self . embed_dim // num_heads ** - 0.5 instance-attribute __init__ ( embed_dim = 768 , num_heads = 12 , attn_drop_p = 0.0 , projection_drop_p = 0.2 , qkv_bias = False ) Source code in glasses/models/vision/backbones/vit/model.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , ): super () . __init__ () self . embed_dim = embed_dim self . num_heads = num_heads # fuse the queries, keys and values in one matrix self . qkv = nn . Linear ( embed_dim , embed_dim * 3 , bias = qkv_bias ) self . att_drop = nn . Dropout ( attn_drop_p ) self . projection = nn . Sequential ( nn . Linear ( embed_dim , embed_dim ), nn . Dropout ( projection_drop_p ) ) self . scaling = ( self . embed_dim // num_heads ) ** - 0.5 forward ( x , mask = None ) Source code in glasses/models/vision/backbones/vit/model.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def forward ( self , x : torch . Tensor , mask : torch . Tensor = None ) -> torch . Tensor : # split keys, queries and values in num_heads qkv = rearrange ( self . qkv ( x ), \"b n (qkv h d) -> (qkv) b h n d\" , h = self . num_heads , qkv = 3 ) queries , keys , values = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] # dot product, Q V^T, here we don't transpose before, so this is why # the sum is made on the last index of K energy = torch . einsum ( \"bhij, bhkj -> bhik\" , queries , keys ) * self . scaling if mask is not None : fill_value = torch . finfo ( torch . float32 ) . min energy . mask_fill ( ~ mask , fill_value ) att = F . softmax ( energy , dim =- 1 ) att = self . att_drop ( att ) # dot product out = torch . einsum ( \"bhij, bhjk -> bhik \" , att , values ) out = rearrange ( out , \"b h n d -> b n (h d)\" ) out = self . projection ( out ) return out ViTBackbone Bases: Backbone Source code in glasses/models/vision/backbones/vit/model.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class ViTBackbone ( Backbone ): def __init__ ( self , img_size : int = 224 , in_channels : int = 3 , patch_size : int = 16 , depth : int = 12 , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . embedder = ViTPatchEmbedding ( in_channels = in_channels , patch_size = patch_size , embed_dim = embed_dim , img_size = img_size ) self . encoder = ViTEncoder ( depth = depth , embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation ) def forward ( self , pixel_values : torch . Tensor ) -> Dict [ str , torch . Tensor ]: embeddings = self . embedder ( pixel_values ) outputs = self . encoder ( embeddings ) return outputs embedder = ViTPatchEmbedding ( in_channels = in_channels , patch_size = patch_size , embed_dim = embed_dim , img_size = img_size ) instance-attribute encoder = ViTEncoder ( depth = depth , embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation ) instance-attribute __init__ ( img_size = 224 , in_channels = 3 , patch_size = 16 , depth = 12 , embed_dim = 768 , num_heads = 12 , attn_drop_p = 0.0 , projection_drop_p = 0.2 , qkv_bias = False , forward_expansion = 4 , forward_drop_p = 0.2 , activation = nn . GELU ) Source code in glasses/models/vision/backbones/vit/model.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def __init__ ( self , img_size : int = 224 , in_channels : int = 3 , patch_size : int = 16 , depth : int = 12 , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . embedder = ViTPatchEmbedding ( in_channels = in_channels , patch_size = patch_size , embed_dim = embed_dim , img_size = img_size ) self . encoder = ViTEncoder ( depth = depth , embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation ) forward ( pixel_values ) Source code in glasses/models/vision/backbones/vit/model.py 237 238 239 240 def forward ( self , pixel_values : torch . Tensor ) -> Dict [ str , torch . Tensor ]: embeddings = self . embedder ( pixel_values ) outputs = self . encoder ( embeddings ) return outputs ViTBlock Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class ViTBlock ( nn . Module ): def __init__ ( self , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . transformer = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTAttentionBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias ) ) ) self . mlp = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTMLPBlock ( embed_dim = embed_dim , expansion = forward_expansion , drop_p = forward_drop_p , activation = activation ) ) ) def forward ( self , x ): x = self . transformer ( x ) x = self . mlp ( x ) return x mlp = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTMLPBlock ( embed_dim = embed_dim , expansion = forward_expansion , drop_p = forward_drop_p , activation = activation ))) instance-attribute transformer = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTAttentionBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias ))) instance-attribute __init__ ( embed_dim = 768 , num_heads = 12 , attn_drop_p = 0.0 , projection_drop_p = 0.2 , qkv_bias = False , forward_expansion = 4 , forward_drop_p = 0.2 , activation = nn . GELU ) Source code in glasses/models/vision/backbones/vit/model.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def __init__ ( self , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . transformer = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTAttentionBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias ) ) ) self . mlp = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTMLPBlock ( embed_dim = embed_dim , expansion = forward_expansion , drop_p = forward_drop_p , activation = activation ) ) ) forward ( x ) Source code in glasses/models/vision/backbones/vit/model.py 160 161 162 163 def forward ( self , x ): x = self . transformer ( x ) x = self . mlp ( x ) return x ViTEncoder Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class ViTEncoder ( nn . Module ): def __init__ ( self , depth : int = 12 , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . layers = nn . ModuleList ( ViTBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation , ) for _ in range ( depth ) ) self . norm = nn . LayerNorm ( embed_dim ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : for layer in self . layers : x = layer ( x ) x = self . norm ( x ) return x layers = nn . ModuleList ( ViTBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation ) for _ in range ( depth )) instance-attribute norm = nn . LayerNorm ( embed_dim ) instance-attribute __init__ ( depth = 12 , embed_dim = 768 , num_heads = 12 , attn_drop_p = 0.0 , projection_drop_p = 0.2 , qkv_bias = False , forward_expansion = 4 , forward_drop_p = 0.2 , activation = nn . GELU ) Source code in glasses/models/vision/backbones/vit/model.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , depth : int = 12 , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . layers = nn . ModuleList ( ViTBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation , ) for _ in range ( depth ) ) self . norm = nn . LayerNorm ( embed_dim ) forward ( x ) Source code in glasses/models/vision/backbones/vit/model.py 193 194 195 196 197 def forward ( self , x : torch . Tensor ) -> torch . Tensor : for layer in self . layers : x = layer ( x ) x = self . norm ( x ) return x ViTMLPBlock Bases: nn . Sequential Source code in glasses/models/vision/backbones/vit/model.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class ViTMLPBlock ( nn . Sequential ): def __init__ ( self , embed_dim : int , expansion : int = 4 , drop_p : float = 0.0 , activation : nn . Module = nn . GELU , ): super () . __init__ ( nn . Linear ( embed_dim , expansion * embed_dim ), activation (), nn . Dropout ( drop_p ), nn . Linear ( expansion * embed_dim , embed_dim ), nn . Dropout ( drop_p ), ) __init__ ( embed_dim , expansion = 4 , drop_p = 0.0 , activation = nn . GELU ) Source code in glasses/models/vision/backbones/vit/model.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , embed_dim : int , expansion : int = 4 , drop_p : float = 0.0 , activation : nn . Module = nn . GELU , ): super () . __init__ ( nn . Linear ( embed_dim , expansion * embed_dim ), activation (), nn . Dropout ( drop_p ), nn . Linear ( expansion * embed_dim , embed_dim ), nn . Dropout ( drop_p ), ) ViTPatchEmbedding Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class ViTPatchEmbedding ( nn . Module ): def __init__ ( self , in_channels : int = 3 , patch_size : int = 16 , embed_dim : int = 768 , img_size : int = 224 , ): super () . __init__ () self . proj = nn . Sequential ( nn . Conv2d ( in_channels , embed_dim , kernel_size = patch_size , stride = patch_size ), Rearrange ( \"b e (h) (w) -> b (h w) e\" ), ) self . tokens = ViTTokens ( embed_dim ) self . positions = nn . Parameter ( torch . randn (( img_size // patch_size ) ** 2 + len ( self . tokens ), embed_dim )) def forward ( self , x : torch . Tensor ) -> torch . Tensor : x = self . proj ( x ) tokens = self . tokens ( x ) x = torch . cat ([ * tokens , x ], dim = 1 ) x = x + self . positions return x positions = nn . Parameter ( torch . randn ( img_size // patch_size ** 2 + len ( self . tokens ), embed_dim )) instance-attribute proj = nn . Sequential ( nn . Conv2d ( in_channels , embed_dim , kernel_size = patch_size , stride = patch_size ), Rearrange ( 'b e (h) (w) -> b (h w) e' )) instance-attribute tokens = ViTTokens ( embed_dim ) instance-attribute __init__ ( in_channels = 3 , patch_size = 16 , embed_dim = 768 , img_size = 224 ) Source code in glasses/models/vision/backbones/vit/model.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , in_channels : int = 3 , patch_size : int = 16 , embed_dim : int = 768 , img_size : int = 224 , ): super () . __init__ () self . proj = nn . Sequential ( nn . Conv2d ( in_channels , embed_dim , kernel_size = patch_size , stride = patch_size ), Rearrange ( \"b e (h) (w) -> b (h w) e\" ), ) self . tokens = ViTTokens ( embed_dim ) self . positions = nn . Parameter ( torch . randn (( img_size // patch_size ) ** 2 + len ( self . tokens ), embed_dim )) forward ( x ) Source code in glasses/models/vision/backbones/vit/model.py 44 45 46 47 48 49 def forward ( self , x : torch . Tensor ) -> torch . Tensor : x = self . proj ( x ) tokens = self . tokens ( x ) x = torch . cat ([ * tokens , x ], dim = 1 ) x = x + self . positions return x ViTTokens Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class ViTTokens ( nn . Module ): def __init__ ( self , embed_dim : int ): super () . __init__ () self . cls_token = nn . Parameter ( torch . randn ( 1 , 1 , embed_dim )) def __len__ ( self ): return len ( list ( self . parameters ())) def forward ( self , x : torch . Tensor ) -> List [ torch . Tensor ]: b = x . shape [ 0 ] tokens = [] for token in self . parameters (): tokens . append ( repeat ( token , \"() n e -> b n e\" , b = b )) return tokens cls_token = nn . Parameter ( torch . randn ( 1 , 1 , embed_dim )) instance-attribute __init__ ( embed_dim ) Source code in glasses/models/vision/backbones/vit/model.py 14 15 16 def __init__ ( self , embed_dim : int ): super () . __init__ () self . cls_token = nn . Parameter ( torch . randn ( 1 , 1 , embed_dim )) __len__ () Source code in glasses/models/vision/backbones/vit/model.py 18 19 def __len__ ( self ): return len ( list ( self . parameters ())) forward ( x ) Source code in glasses/models/vision/backbones/vit/model.py 21 22 23 24 25 26 def forward ( self , x : torch . Tensor ) -> List [ torch . Tensor ]: b = x . shape [ 0 ] tokens = [] for token in self . parameters (): tokens . append ( repeat ( token , \"() n e -> b n e\" , b = b )) return tokens","title":"model"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ResidualAddition","text":"Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 112 113 114 115 116 117 118 119 120 class ResidualAddition ( nn . Module ): def __init__ ( self , fn ): super () . __init__ () self . fn = fn def forward ( self , x , ** kwargs ): out = self . fn ( x , ** kwargs ) x = x + out return x","title":"ResidualAddition"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ResidualAddition.fn","text":"","title":"fn"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ResidualAddition.__init__","text":"Source code in glasses/models/vision/backbones/vit/model.py 113 114 115 def __init__ ( self , fn ): super () . __init__ () self . fn = fn","title":"__init__()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ResidualAddition.forward","text":"Source code in glasses/models/vision/backbones/vit/model.py 117 118 119 120 def forward ( self , x , ** kwargs ): out = self . fn ( x , ** kwargs ) x = x + out return x","title":"forward()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTAttentionBlock","text":"Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class ViTAttentionBlock ( nn . Module ): def __init__ ( self , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , ): super () . __init__ () self . embed_dim = embed_dim self . num_heads = num_heads # fuse the queries, keys and values in one matrix self . qkv = nn . Linear ( embed_dim , embed_dim * 3 , bias = qkv_bias ) self . att_drop = nn . Dropout ( attn_drop_p ) self . projection = nn . Sequential ( nn . Linear ( embed_dim , embed_dim ), nn . Dropout ( projection_drop_p ) ) self . scaling = ( self . embed_dim // num_heads ) ** - 0.5 def forward ( self , x : torch . Tensor , mask : torch . Tensor = None ) -> torch . Tensor : # split keys, queries and values in num_heads qkv = rearrange ( self . qkv ( x ), \"b n (qkv h d) -> (qkv) b h n d\" , h = self . num_heads , qkv = 3 ) queries , keys , values = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] # dot product, Q V^T, here we don't transpose before, so this is why # the sum is made on the last index of K energy = torch . einsum ( \"bhij, bhkj -> bhik\" , queries , keys ) * self . scaling if mask is not None : fill_value = torch . finfo ( torch . float32 ) . min energy . mask_fill ( ~ mask , fill_value ) att = F . softmax ( energy , dim =- 1 ) att = self . att_drop ( att ) # dot product out = torch . einsum ( \"bhij, bhjk -> bhik \" , att , values ) out = rearrange ( out , \"b h n d -> b n (h d)\" ) out = self . projection ( out ) return out","title":"ViTAttentionBlock"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTAttentionBlock.att_drop","text":"","title":"att_drop"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTAttentionBlock.embed_dim","text":"","title":"embed_dim"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTAttentionBlock.num_heads","text":"","title":"num_heads"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTAttentionBlock.projection","text":"","title":"projection"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTAttentionBlock.qkv","text":"","title":"qkv"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTAttentionBlock.scaling","text":"","title":"scaling"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTAttentionBlock.__init__","text":"Source code in glasses/models/vision/backbones/vit/model.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , ): super () . __init__ () self . embed_dim = embed_dim self . num_heads = num_heads # fuse the queries, keys and values in one matrix self . qkv = nn . Linear ( embed_dim , embed_dim * 3 , bias = qkv_bias ) self . att_drop = nn . Dropout ( attn_drop_p ) self . projection = nn . Sequential ( nn . Linear ( embed_dim , embed_dim ), nn . Dropout ( projection_drop_p ) ) self . scaling = ( self . embed_dim // num_heads ) ** - 0.5","title":"__init__()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTAttentionBlock.forward","text":"Source code in glasses/models/vision/backbones/vit/model.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def forward ( self , x : torch . Tensor , mask : torch . Tensor = None ) -> torch . Tensor : # split keys, queries and values in num_heads qkv = rearrange ( self . qkv ( x ), \"b n (qkv h d) -> (qkv) b h n d\" , h = self . num_heads , qkv = 3 ) queries , keys , values = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] # dot product, Q V^T, here we don't transpose before, so this is why # the sum is made on the last index of K energy = torch . einsum ( \"bhij, bhkj -> bhik\" , queries , keys ) * self . scaling if mask is not None : fill_value = torch . finfo ( torch . float32 ) . min energy . mask_fill ( ~ mask , fill_value ) att = F . softmax ( energy , dim =- 1 ) att = self . att_drop ( att ) # dot product out = torch . einsum ( \"bhij, bhjk -> bhik \" , att , values ) out = rearrange ( out , \"b h n d -> b n (h d)\" ) out = self . projection ( out ) return out","title":"forward()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBackbone","text":"Bases: Backbone Source code in glasses/models/vision/backbones/vit/model.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class ViTBackbone ( Backbone ): def __init__ ( self , img_size : int = 224 , in_channels : int = 3 , patch_size : int = 16 , depth : int = 12 , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . embedder = ViTPatchEmbedding ( in_channels = in_channels , patch_size = patch_size , embed_dim = embed_dim , img_size = img_size ) self . encoder = ViTEncoder ( depth = depth , embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation ) def forward ( self , pixel_values : torch . Tensor ) -> Dict [ str , torch . Tensor ]: embeddings = self . embedder ( pixel_values ) outputs = self . encoder ( embeddings ) return outputs","title":"ViTBackbone"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBackbone.embedder","text":"","title":"embedder"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBackbone.encoder","text":"","title":"encoder"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBackbone.__init__","text":"Source code in glasses/models/vision/backbones/vit/model.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def __init__ ( self , img_size : int = 224 , in_channels : int = 3 , patch_size : int = 16 , depth : int = 12 , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . embedder = ViTPatchEmbedding ( in_channels = in_channels , patch_size = patch_size , embed_dim = embed_dim , img_size = img_size ) self . encoder = ViTEncoder ( depth = depth , embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation )","title":"__init__()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBackbone.forward","text":"Source code in glasses/models/vision/backbones/vit/model.py 237 238 239 240 def forward ( self , pixel_values : torch . Tensor ) -> Dict [ str , torch . Tensor ]: embeddings = self . embedder ( pixel_values ) outputs = self . encoder ( embeddings ) return outputs","title":"forward()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBlock","text":"Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class ViTBlock ( nn . Module ): def __init__ ( self , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . transformer = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTAttentionBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias ) ) ) self . mlp = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTMLPBlock ( embed_dim = embed_dim , expansion = forward_expansion , drop_p = forward_drop_p , activation = activation ) ) ) def forward ( self , x ): x = self . transformer ( x ) x = self . mlp ( x ) return x","title":"ViTBlock"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBlock.mlp","text":"","title":"mlp"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBlock.transformer","text":"","title":"transformer"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBlock.__init__","text":"Source code in glasses/models/vision/backbones/vit/model.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def __init__ ( self , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . transformer = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTAttentionBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias ) ) ) self . mlp = ResidualAddition ( nn . Sequential ( nn . LayerNorm ( embed_dim ), ViTMLPBlock ( embed_dim = embed_dim , expansion = forward_expansion , drop_p = forward_drop_p , activation = activation ) ) )","title":"__init__()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTBlock.forward","text":"Source code in glasses/models/vision/backbones/vit/model.py 160 161 162 163 def forward ( self , x ): x = self . transformer ( x ) x = self . mlp ( x ) return x","title":"forward()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTEncoder","text":"Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class ViTEncoder ( nn . Module ): def __init__ ( self , depth : int = 12 , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . layers = nn . ModuleList ( ViTBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation , ) for _ in range ( depth ) ) self . norm = nn . LayerNorm ( embed_dim ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : for layer in self . layers : x = layer ( x ) x = self . norm ( x ) return x","title":"ViTEncoder"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTEncoder.layers","text":"","title":"layers"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTEncoder.norm","text":"","title":"norm"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTEncoder.__init__","text":"Source code in glasses/models/vision/backbones/vit/model.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , depth : int = 12 , embed_dim : int = 768 , num_heads : int = 12 , attn_drop_p : float = 0.0 , projection_drop_p : float = 0.2 , qkv_bias : bool = False , forward_expansion : int = 4 , forward_drop_p : float = 0.2 , activation : nn . Module = nn . GELU , ): super () . __init__ () self . layers = nn . ModuleList ( ViTBlock ( embed_dim = embed_dim , num_heads = num_heads , attn_drop_p = attn_drop_p , projection_drop_p = projection_drop_p , qkv_bias = qkv_bias , forward_expansion = forward_expansion , forward_drop_p = forward_drop_p , activation = activation , ) for _ in range ( depth ) ) self . norm = nn . LayerNorm ( embed_dim )","title":"__init__()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTEncoder.forward","text":"Source code in glasses/models/vision/backbones/vit/model.py 193 194 195 196 197 def forward ( self , x : torch . Tensor ) -> torch . Tensor : for layer in self . layers : x = layer ( x ) x = self . norm ( x ) return x","title":"forward()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTMLPBlock","text":"Bases: nn . Sequential Source code in glasses/models/vision/backbones/vit/model.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class ViTMLPBlock ( nn . Sequential ): def __init__ ( self , embed_dim : int , expansion : int = 4 , drop_p : float = 0.0 , activation : nn . Module = nn . GELU , ): super () . __init__ ( nn . Linear ( embed_dim , expansion * embed_dim ), activation (), nn . Dropout ( drop_p ), nn . Linear ( expansion * embed_dim , embed_dim ), nn . Dropout ( drop_p ), )","title":"ViTMLPBlock"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTMLPBlock.__init__","text":"Source code in glasses/models/vision/backbones/vit/model.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , embed_dim : int , expansion : int = 4 , drop_p : float = 0.0 , activation : nn . Module = nn . GELU , ): super () . __init__ ( nn . Linear ( embed_dim , expansion * embed_dim ), activation (), nn . Dropout ( drop_p ), nn . Linear ( expansion * embed_dim , embed_dim ), nn . Dropout ( drop_p ), )","title":"__init__()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTPatchEmbedding","text":"Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class ViTPatchEmbedding ( nn . Module ): def __init__ ( self , in_channels : int = 3 , patch_size : int = 16 , embed_dim : int = 768 , img_size : int = 224 , ): super () . __init__ () self . proj = nn . Sequential ( nn . Conv2d ( in_channels , embed_dim , kernel_size = patch_size , stride = patch_size ), Rearrange ( \"b e (h) (w) -> b (h w) e\" ), ) self . tokens = ViTTokens ( embed_dim ) self . positions = nn . Parameter ( torch . randn (( img_size // patch_size ) ** 2 + len ( self . tokens ), embed_dim )) def forward ( self , x : torch . Tensor ) -> torch . Tensor : x = self . proj ( x ) tokens = self . tokens ( x ) x = torch . cat ([ * tokens , x ], dim = 1 ) x = x + self . positions return x","title":"ViTPatchEmbedding"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTPatchEmbedding.positions","text":"","title":"positions"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTPatchEmbedding.proj","text":"","title":"proj"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTPatchEmbedding.tokens","text":"","title":"tokens"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTPatchEmbedding.__init__","text":"Source code in glasses/models/vision/backbones/vit/model.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , in_channels : int = 3 , patch_size : int = 16 , embed_dim : int = 768 , img_size : int = 224 , ): super () . __init__ () self . proj = nn . Sequential ( nn . Conv2d ( in_channels , embed_dim , kernel_size = patch_size , stride = patch_size ), Rearrange ( \"b e (h) (w) -> b (h w) e\" ), ) self . tokens = ViTTokens ( embed_dim ) self . positions = nn . Parameter ( torch . randn (( img_size // patch_size ) ** 2 + len ( self . tokens ), embed_dim ))","title":"__init__()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTPatchEmbedding.forward","text":"Source code in glasses/models/vision/backbones/vit/model.py 44 45 46 47 48 49 def forward ( self , x : torch . Tensor ) -> torch . Tensor : x = self . proj ( x ) tokens = self . tokens ( x ) x = torch . cat ([ * tokens , x ], dim = 1 ) x = x + self . positions return x","title":"forward()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTTokens","text":"Bases: nn . Module Source code in glasses/models/vision/backbones/vit/model.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class ViTTokens ( nn . Module ): def __init__ ( self , embed_dim : int ): super () . __init__ () self . cls_token = nn . Parameter ( torch . randn ( 1 , 1 , embed_dim )) def __len__ ( self ): return len ( list ( self . parameters ())) def forward ( self , x : torch . Tensor ) -> List [ torch . Tensor ]: b = x . shape [ 0 ] tokens = [] for token in self . parameters (): tokens . append ( repeat ( token , \"() n e -> b n e\" , b = b )) return tokens","title":"ViTTokens"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTTokens.cls_token","text":"","title":"cls_token"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTTokens.__init__","text":"Source code in glasses/models/vision/backbones/vit/model.py 14 15 16 def __init__ ( self , embed_dim : int ): super () . __init__ () self . cls_token = nn . Parameter ( torch . randn ( 1 , 1 , embed_dim ))","title":"__init__()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTTokens.__len__","text":"Source code in glasses/models/vision/backbones/vit/model.py 18 19 def __len__ ( self ): return len ( list ( self . parameters ()))","title":"__len__()"},{"location":"reference/models/vision/backbones/vit/model/#glasses.models.vision.backbones.vit.model.ViTTokens.forward","text":"Source code in glasses/models/vision/backbones/vit/model.py 21 22 23 24 25 26 def forward ( self , x : torch . Tensor ) -> List [ torch . Tensor ]: b = x . shape [ 0 ] tokens = [] for token in self . parameters (): tokens . append ( repeat ( token , \"() n e -> b n e\" , b = b )) return tokens","title":"forward()"},{"location":"reference/models/vision/backbones/vit/zoo/","text":"zoo = ModelZoo ( vit_small_patch16_224 = vit_small_patch16_224 , vit_base_patch16_224 = vit_base_patch16_224 , vit_base_patch16_384 = vit_base_patch16_384 ) module-attribute vit_base_patch16_224 () Source code in glasses/models/vision/backbones/vit/zoo.py 7 8 def vit_base_patch16_224 (): return ViTBackboneConfig ( depth = 12 , num_heads = 12 , forward_expansion = 4 , qkv_bias = True ) vit_base_patch16_384 () Source code in glasses/models/vision/backbones/vit/zoo.py 10 11 def vit_base_patch16_384 (): return ViTBackboneConfig ( img_size = 384 , depth = 12 , num_heads = 12 , forward_expansion = 4 , qkv_bias = True ) vit_small_patch16_224 () Source code in glasses/models/vision/backbones/vit/zoo.py 4 5 def vit_small_patch16_224 (): return ViTBackboneConfig ( depth = 8 , num_heads = 8 , forward_expansion = 3 )","title":"zoo"},{"location":"reference/models/vision/backbones/vit/zoo/#glasses.models.vision.backbones.vit.zoo.zoo","text":"","title":"zoo"},{"location":"reference/models/vision/backbones/vit/zoo/#glasses.models.vision.backbones.vit.zoo.vit_base_patch16_224","text":"Source code in glasses/models/vision/backbones/vit/zoo.py 7 8 def vit_base_patch16_224 (): return ViTBackboneConfig ( depth = 12 , num_heads = 12 , forward_expansion = 4 , qkv_bias = True )","title":"vit_base_patch16_224()"},{"location":"reference/models/vision/backbones/vit/zoo/#glasses.models.vision.backbones.vit.zoo.vit_base_patch16_384","text":"Source code in glasses/models/vision/backbones/vit/zoo.py 10 11 def vit_base_patch16_384 (): return ViTBackboneConfig ( img_size = 384 , depth = 12 , num_heads = 12 , forward_expansion = 4 , qkv_bias = True )","title":"vit_base_patch16_384()"},{"location":"reference/models/vision/backbones/vit/zoo/#glasses.models.vision.backbones.vit.zoo.vit_small_patch16_224","text":"Source code in glasses/models/vision/backbones/vit/zoo.py 4 5 def vit_small_patch16_224 (): return ViTBackboneConfig ( depth = 8 , num_heads = 8 , forward_expansion = 3 )","title":"vit_small_patch16_224()"},{"location":"reference/models/vision/classification/","text":"","title":"Index"},{"location":"reference/models/vision/classification/base/","text":"ModelForClassification Bases: nn . Module Base class for classification models Define a custom classification model. It can be whatever you want, the only contrain is that it must return a ModelForClassificationOutput . class MyModelForClassification ( ModelForClassification ): def __init__ ( self , in_channels : int , num_classes : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , 64 , kernel_size = 3 ) self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( 64 , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : x = self . conv ( pixel_values ) x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return { \"logits\" : x } The above example is a fixed models, it doesn't have composable part. In reality, classification models are (usually) composed by a backbone and a head . Since all the backbones and heads in glasses must follow known rules, it trivial to compose them. from glasses.models.vision.backbones import ResNet class ResNetForClassification ( ModelForClassification ): def __init__ ( self , in_channels : int , ... , num_classes : int ): super () . __init__ () self . backbone = ResNet ( in_channels , .... ) self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( 64 , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : features = self . backbone ( pixel_values ) x = features [ - 1 ] x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return { \"logits\" : x } In 99% of cases you will take advantage of the AnyModelForClassification that allows you to mix on the fly any backbone and classification head in glasses. Source code in glasses/models/vision/classification/base.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class ModelForClassification ( nn . Module ): \"\"\"Base class for classification models Define a custom classification model. It can be whatever you want, the only contrain is that it **must** return a `ModelForClassificationOutput`. ```python class MyModelForClassification(ModelForClassification): def __init__(self, in_channels: int, num_classes: int ): super().__init__() self.conv = nn.Conv2d(in_channels, 64, kernel_size=3) self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(64, num_classes) def forward(self, pixel_values: Tensor) -> ModelForClassificationOutput: x = self.conv(pixel_values) x = self.pool(x) x = self.flat(x) x = self.fc(x) return {\"logits\": x} ``` The above example is a fixed models, it doesn't have composable part. In reality, classification models are (usually) composed by a **backbone** and a **head**. Since all the backbones and heads in glasses must follow known rules, it trivial to compose them. ```python from glasses.models.vision.backbones import ResNet class ResNetForClassification(ModelForClassification): def __init__(self, in_channels: int, ..., num_classes: int): super().__init__() self.backbone =ResNet(in_channels, ....) self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(64, num_classes) def forward(self, pixel_values: Tensor) -> ModelForClassificationOutput: features = self.backbone(pixel_values) x = features[-1] x = self.pool(x) x = self.flat(x) x = self.fc(x) return {\"logits\": x} ``` In 99% of cases you will take advantage of the [`AnyModelForClassification`]() that allows you to mix on the fly any backbone and classification head in glasses. \"\"\" def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : \"\"\"The forward method for classification head. Args: pixel_values (Tensor): The input image. Returns: Tensor: The logits. \"\"\" raise NotImplemented forward ( pixel_values ) The forward method for classification head. Parameters: Name Type Description Default pixel_values Tensor The input image. required Returns: Name Type Description Tensor ModelForClassificationOutput The logits. Source code in glasses/models/vision/classification/base.py 61 62 63 64 65 66 67 68 69 70 def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : \"\"\"The forward method for classification head. Args: pixel_values (Tensor): The input image. Returns: Tensor: The logits. \"\"\" raise NotImplemented","title":"base"},{"location":"reference/models/vision/classification/base/#glasses.models.vision.classification.base.ModelForClassification","text":"Bases: nn . Module Base class for classification models Define a custom classification model. It can be whatever you want, the only contrain is that it must return a ModelForClassificationOutput . class MyModelForClassification ( ModelForClassification ): def __init__ ( self , in_channels : int , num_classes : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , 64 , kernel_size = 3 ) self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( 64 , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : x = self . conv ( pixel_values ) x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return { \"logits\" : x } The above example is a fixed models, it doesn't have composable part. In reality, classification models are (usually) composed by a backbone and a head . Since all the backbones and heads in glasses must follow known rules, it trivial to compose them. from glasses.models.vision.backbones import ResNet class ResNetForClassification ( ModelForClassification ): def __init__ ( self , in_channels : int , ... , num_classes : int ): super () . __init__ () self . backbone = ResNet ( in_channels , .... ) self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( 64 , num_classes ) def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : features = self . backbone ( pixel_values ) x = features [ - 1 ] x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return { \"logits\" : x } In 99% of cases you will take advantage of the AnyModelForClassification that allows you to mix on the fly any backbone and classification head in glasses. Source code in glasses/models/vision/classification/base.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class ModelForClassification ( nn . Module ): \"\"\"Base class for classification models Define a custom classification model. It can be whatever you want, the only contrain is that it **must** return a `ModelForClassificationOutput`. ```python class MyModelForClassification(ModelForClassification): def __init__(self, in_channels: int, num_classes: int ): super().__init__() self.conv = nn.Conv2d(in_channels, 64, kernel_size=3) self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(64, num_classes) def forward(self, pixel_values: Tensor) -> ModelForClassificationOutput: x = self.conv(pixel_values) x = self.pool(x) x = self.flat(x) x = self.fc(x) return {\"logits\": x} ``` The above example is a fixed models, it doesn't have composable part. In reality, classification models are (usually) composed by a **backbone** and a **head**. Since all the backbones and heads in glasses must follow known rules, it trivial to compose them. ```python from glasses.models.vision.backbones import ResNet class ResNetForClassification(ModelForClassification): def __init__(self, in_channels: int, ..., num_classes: int): super().__init__() self.backbone =ResNet(in_channels, ....) self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(64, num_classes) def forward(self, pixel_values: Tensor) -> ModelForClassificationOutput: features = self.backbone(pixel_values) x = features[-1] x = self.pool(x) x = self.flat(x) x = self.fc(x) return {\"logits\": x} ``` In 99% of cases you will take advantage of the [`AnyModelForClassification`]() that allows you to mix on the fly any backbone and classification head in glasses. \"\"\" def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : \"\"\"The forward method for classification head. Args: pixel_values (Tensor): The input image. Returns: Tensor: The logits. \"\"\" raise NotImplemented","title":"ModelForClassification"},{"location":"reference/models/vision/classification/base/#glasses.models.vision.classification.base.ModelForClassification.forward","text":"The forward method for classification head. Parameters: Name Type Description Default pixel_values Tensor The input image. required Returns: Name Type Description Tensor ModelForClassificationOutput The logits. Source code in glasses/models/vision/classification/base.py 61 62 63 64 65 66 67 68 69 70 def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : \"\"\"The forward method for classification head. Args: pixel_values (Tensor): The input image. Returns: Tensor: The logits. \"\"\" raise NotImplemented","title":"forward()"},{"location":"reference/models/vision/classification/outputs/","text":"ModelForClassificationOutput Bases: TypedDict The output for image classification models. Source code in glasses/models/vision/classification/outputs.py 6 7 8 9 class ModelForClassificationOutput ( TypedDict ): \"\"\"The output for image classification models.\"\"\" logits : Tensor logits : Tensor class-attribute","title":"outputs"},{"location":"reference/models/vision/classification/outputs/#glasses.models.vision.classification.outputs.ModelForClassificationOutput","text":"Bases: TypedDict The output for image classification models. Source code in glasses/models/vision/classification/outputs.py 6 7 8 9 class ModelForClassificationOutput ( TypedDict ): \"\"\"The output for image classification models.\"\"\" logits : Tensor","title":"ModelForClassificationOutput"},{"location":"reference/models/vision/classification/outputs/#glasses.models.vision.classification.outputs.ModelForClassificationOutput.logits","text":"","title":"logits"},{"location":"reference/models/vision/classification/common/","text":"","title":"Index"},{"location":"reference/models/vision/classification/common/config/","text":"AnyModelForClassificationConfig dataclass Bases: Config Source code in glasses/models/vision/classification/common/config.py 8 9 10 11 12 13 14 15 16 @dataclass class AnyModelForClassificationConfig ( Config ): backbone_config : Config head_config : Config def build ( self ) -> AnyModelForClassification : backbone = self . backbone_config . build () head = self . head_config . build () return AnyModelForClassification ( backbone , head ) backbone_config : Config class-attribute head_config : Config class-attribute build () Source code in glasses/models/vision/classification/common/config.py 13 14 15 16 def build ( self ) -> AnyModelForClassification : backbone = self . backbone_config . build () head = self . head_config . build () return AnyModelForClassification ( backbone , head )","title":"config"},{"location":"reference/models/vision/classification/common/config/#glasses.models.vision.classification.common.config.AnyModelForClassificationConfig","text":"Bases: Config Source code in glasses/models/vision/classification/common/config.py 8 9 10 11 12 13 14 15 16 @dataclass class AnyModelForClassificationConfig ( Config ): backbone_config : Config head_config : Config def build ( self ) -> AnyModelForClassification : backbone = self . backbone_config . build () head = self . head_config . build () return AnyModelForClassification ( backbone , head )","title":"AnyModelForClassificationConfig"},{"location":"reference/models/vision/classification/common/config/#glasses.models.vision.classification.common.config.AnyModelForClassificationConfig.backbone_config","text":"","title":"backbone_config"},{"location":"reference/models/vision/classification/common/config/#glasses.models.vision.classification.common.config.AnyModelForClassificationConfig.head_config","text":"","title":"head_config"},{"location":"reference/models/vision/classification/common/config/#glasses.models.vision.classification.common.config.AnyModelForClassificationConfig.build","text":"Source code in glasses/models/vision/classification/common/config.py 13 14 15 16 def build ( self ) -> AnyModelForClassification : backbone = self . backbone_config . build () head = self . head_config . build () return AnyModelForClassification ( backbone , head )","title":"build()"},{"location":"reference/models/vision/classification/common/model/","text":"AnyModelForClassification Bases: ModelForClassification Source code in glasses/models/vision/classification/common/model.py 11 12 13 14 15 16 17 18 19 20 class AnyModelForClassification ( ModelForClassification ): def __init__ ( self , backbone : nn . Module , head : nn . Module ): super () . __init__ () self . backbone = backbone self . head = head def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : features : List [ Tensor ] = self . backbone ( pixel_values ) logits : Tensor = self . head ( features ) return { \"logits\" : logits } backbone = backbone instance-attribute head = head instance-attribute __init__ ( backbone , head ) Source code in glasses/models/vision/classification/common/model.py 12 13 14 15 def __init__ ( self , backbone : nn . Module , head : nn . Module ): super () . __init__ () self . backbone = backbone self . head = head forward ( pixel_values ) Source code in glasses/models/vision/classification/common/model.py 17 18 19 20 def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : features : List [ Tensor ] = self . backbone ( pixel_values ) logits : Tensor = self . head ( features ) return { \"logits\" : logits }","title":"model"},{"location":"reference/models/vision/classification/common/model/#glasses.models.vision.classification.common.model.AnyModelForClassification","text":"Bases: ModelForClassification Source code in glasses/models/vision/classification/common/model.py 11 12 13 14 15 16 17 18 19 20 class AnyModelForClassification ( ModelForClassification ): def __init__ ( self , backbone : nn . Module , head : nn . Module ): super () . __init__ () self . backbone = backbone self . head = head def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : features : List [ Tensor ] = self . backbone ( pixel_values ) logits : Tensor = self . head ( features ) return { \"logits\" : logits }","title":"AnyModelForClassification"},{"location":"reference/models/vision/classification/common/model/#glasses.models.vision.classification.common.model.AnyModelForClassification.backbone","text":"","title":"backbone"},{"location":"reference/models/vision/classification/common/model/#glasses.models.vision.classification.common.model.AnyModelForClassification.head","text":"","title":"head"},{"location":"reference/models/vision/classification/common/model/#glasses.models.vision.classification.common.model.AnyModelForClassification.__init__","text":"Source code in glasses/models/vision/classification/common/model.py 12 13 14 15 def __init__ ( self , backbone : nn . Module , head : nn . Module ): super () . __init__ () self . backbone = backbone self . head = head","title":"__init__()"},{"location":"reference/models/vision/classification/common/model/#glasses.models.vision.classification.common.model.AnyModelForClassification.forward","text":"Source code in glasses/models/vision/classification/common/model.py 17 18 19 20 def forward ( self , pixel_values : Tensor ) -> ModelForClassificationOutput : features : List [ Tensor ] = self . backbone ( pixel_values ) logits : Tensor = self . head ( features ) return { \"logits\" : logits }","title":"forward()"},{"location":"reference/models/vision/classification/heads/","text":"","title":"Index"},{"location":"reference/models/vision/classification/heads/base/","text":"HeadForClassification Bases: nn . Module Base class for classification heads Define a custom classification head class LinearHead ( HeadForClassification ): def __init__ ( self , num_classes : int , in_channels : int ): super () . __init__ () self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( in_channels , num_classes ) def forward ( self , features : List [ Tensor ]) -> Tensor : x = features [ - 1 ] x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return x Source code in glasses/models/vision/classification/heads/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class HeadForClassification ( nn . Module ): \"\"\"Base class for classification heads Define a custom classification head ```python class LinearHead(HeadForClassification): def __init__(self, num_classes: int, in_channels: int): super().__init__() self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(in_channels, num_classes) def forward(self, features: List[Tensor]) -> Tensor: x = features[-1] x = self.pool(x) x = self.flat(x) x = self.fc(x) return x ``` \"\"\" def forward ( self , features : List [ Tensor ]) -> Tensor : \"\"\"The forward method for classification head. Args: features (List[Tensor]): A list of features. Returns: Tensor: The logits \"\"\" raise NotImplemented forward ( features ) The forward method for classification head. Parameters: Name Type Description Default features List [ Tensor ] A list of features. required Returns: Name Type Description Tensor Tensor The logits Source code in glasses/models/vision/classification/heads/base.py 30 31 32 33 34 35 36 37 38 39 def forward ( self , features : List [ Tensor ]) -> Tensor : \"\"\"The forward method for classification head. Args: features (List[Tensor]): A list of features. Returns: Tensor: The logits \"\"\" raise NotImplemented","title":"base"},{"location":"reference/models/vision/classification/heads/base/#glasses.models.vision.classification.heads.base.HeadForClassification","text":"Bases: nn . Module Base class for classification heads Define a custom classification head class LinearHead ( HeadForClassification ): def __init__ ( self , num_classes : int , in_channels : int ): super () . __init__ () self . pool = nn . AdaptiveAvgPool2d (( 1 , 1 )) self . flat = nn . Flatten () self . fc = nn . Linear ( in_channels , num_classes ) def forward ( self , features : List [ Tensor ]) -> Tensor : x = features [ - 1 ] x = self . pool ( x ) x = self . flat ( x ) x = self . fc ( x ) return x Source code in glasses/models/vision/classification/heads/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class HeadForClassification ( nn . Module ): \"\"\"Base class for classification heads Define a custom classification head ```python class LinearHead(HeadForClassification): def __init__(self, num_classes: int, in_channels: int): super().__init__() self.pool = nn.AdaptiveAvgPool2d((1, 1)) self.flat = nn.Flatten() self.fc = nn.Linear(in_channels, num_classes) def forward(self, features: List[Tensor]) -> Tensor: x = features[-1] x = self.pool(x) x = self.flat(x) x = self.fc(x) return x ``` \"\"\" def forward ( self , features : List [ Tensor ]) -> Tensor : \"\"\"The forward method for classification head. Args: features (List[Tensor]): A list of features. Returns: Tensor: The logits \"\"\" raise NotImplemented","title":"HeadForClassification"},{"location":"reference/models/vision/classification/heads/base/#glasses.models.vision.classification.heads.base.HeadForClassification.forward","text":"The forward method for classification head. Parameters: Name Type Description Default features List [ Tensor ] A list of features. required Returns: Name Type Description Tensor Tensor The logits Source code in glasses/models/vision/classification/heads/base.py 30 31 32 33 34 35 36 37 38 39 def forward ( self , features : List [ Tensor ]) -> Tensor : \"\"\"The forward method for classification head. Args: features (List[Tensor]): A list of features. Returns: Tensor: The logits \"\"\" raise NotImplemented","title":"forward()"},{"location":"reference/models/vision/classification/heads/linear_head/","text":"","title":"Index"},{"location":"reference/models/vision/classification/heads/linear_head/config/","text":"LinearHeadConfig dataclass Bases: Config Source code in glasses/models/vision/classification/heads/linear_head/config.py 8 9 10 11 12 13 14 @dataclass class LinearHeadConfig ( Config ): in_channels : int num_classes : int def build ( self ): return LinearHead ( ** self . __dict__ ) in_channels : int class-attribute num_classes : int class-attribute build () Source code in glasses/models/vision/classification/heads/linear_head/config.py 13 14 def build ( self ): return LinearHead ( ** self . __dict__ )","title":"config"},{"location":"reference/models/vision/classification/heads/linear_head/config/#glasses.models.vision.classification.heads.linear_head.config.LinearHeadConfig","text":"Bases: Config Source code in glasses/models/vision/classification/heads/linear_head/config.py 8 9 10 11 12 13 14 @dataclass class LinearHeadConfig ( Config ): in_channels : int num_classes : int def build ( self ): return LinearHead ( ** self . __dict__ )","title":"LinearHeadConfig"},{"location":"reference/models/vision/classification/heads/linear_head/config/#glasses.models.vision.classification.heads.linear_head.config.LinearHeadConfig.in_channels","text":"","title":"in_channels"},{"location":"reference/models/vision/classification/heads/linear_head/config/#glasses.models.vision.classification.heads.linear_head.config.LinearHeadConfig.num_classes","text":"","title":"num_classes"},{"location":"reference/models/vision/classification/heads/linear_head/config/#glasses.models.vision.classification.heads.linear_head.config.LinearHeadConfig.build","text":"Source code in glasses/models/vision/classification/heads/linear_head/config.py 13 14 def build ( self ): return LinearHead ( ** self . __dict__ )","title":"build()"},{"location":"reference/models/vision/classification/heads/linear_head/model/","text":"LinearHead Bases: HeadForClassification Source code in glasses/models/vision/classification/heads/linear_head/model.py 4 5 6 7 8 9 10 11 12 13 14 15 class LinearHead ( HeadForClassification ): def __init__ ( self , in_channels : int , num_classes : int , ): super () . __init__ () self . fc = nn . Linear ( in_channels , num_classes ) def forward ( self , features : Tensor ) -> Tensor : logits = self . fc ( features ) return logits fc = nn . Linear ( in_channels , num_classes ) instance-attribute __init__ ( in_channels , num_classes ) Source code in glasses/models/vision/classification/heads/linear_head/model.py 5 6 7 8 9 10 11 def __init__ ( self , in_channels : int , num_classes : int , ): super () . __init__ () self . fc = nn . Linear ( in_channels , num_classes ) forward ( features ) Source code in glasses/models/vision/classification/heads/linear_head/model.py 13 14 15 def forward ( self , features : Tensor ) -> Tensor : logits = self . fc ( features ) return logits","title":"model"},{"location":"reference/models/vision/classification/heads/linear_head/model/#glasses.models.vision.classification.heads.linear_head.model.LinearHead","text":"Bases: HeadForClassification Source code in glasses/models/vision/classification/heads/linear_head/model.py 4 5 6 7 8 9 10 11 12 13 14 15 class LinearHead ( HeadForClassification ): def __init__ ( self , in_channels : int , num_classes : int , ): super () . __init__ () self . fc = nn . Linear ( in_channels , num_classes ) def forward ( self , features : Tensor ) -> Tensor : logits = self . fc ( features ) return logits","title":"LinearHead"},{"location":"reference/models/vision/classification/heads/linear_head/model/#glasses.models.vision.classification.heads.linear_head.model.LinearHead.fc","text":"","title":"fc"},{"location":"reference/models/vision/classification/heads/linear_head/model/#glasses.models.vision.classification.heads.linear_head.model.LinearHead.__init__","text":"Source code in glasses/models/vision/classification/heads/linear_head/model.py 5 6 7 8 9 10 11 def __init__ ( self , in_channels : int , num_classes : int , ): super () . __init__ () self . fc = nn . Linear ( in_channels , num_classes )","title":"__init__()"},{"location":"reference/models/vision/classification/heads/linear_head/model/#glasses.models.vision.classification.heads.linear_head.model.LinearHead.forward","text":"Source code in glasses/models/vision/classification/heads/linear_head/model.py 13 14 15 def forward ( self , features : Tensor ) -> Tensor : logits = self . fc ( features ) return logits","title":"forward()"},{"location":"reference/models/vision/classification/vit/","text":"","title":"Index"},{"location":"reference/models/vision/classification/vit/config/","text":"ViTForClassificationConfig dataclass Bases: AnyModelForClassificationConfig Config for ViT model Source code in glasses/models/vision/classification/vit/config.py 9 10 11 12 13 14 @dataclass class ViTForClassificationConfig ( AnyModelForClassificationConfig ): \"\"\"Config for [`ViT`](/models/vision/classification/vit) model\"\"\" backbone_config : ViTBackboneConfig head_config : LinearHeadConfig backbone_config : ViTBackboneConfig class-attribute head_config : LinearHeadConfig class-attribute","title":"config"},{"location":"reference/models/vision/classification/vit/config/#glasses.models.vision.classification.vit.config.ViTForClassificationConfig","text":"Bases: AnyModelForClassificationConfig Config for ViT model Source code in glasses/models/vision/classification/vit/config.py 9 10 11 12 13 14 @dataclass class ViTForClassificationConfig ( AnyModelForClassificationConfig ): \"\"\"Config for [`ViT`](/models/vision/classification/vit) model\"\"\" backbone_config : ViTBackboneConfig head_config : LinearHeadConfig","title":"ViTForClassificationConfig"},{"location":"reference/models/vision/classification/vit/config/#glasses.models.vision.classification.vit.config.ViTForClassificationConfig.backbone_config","text":"","title":"backbone_config"},{"location":"reference/models/vision/classification/vit/config/#glasses.models.vision.classification.vit.config.ViTForClassificationConfig.head_config","text":"","title":"head_config"},{"location":"reference/models/vision/classification/vit/zoo/","text":"zoo = ModelZoo ( vit_small_patch16_224 = vit_small_patch16_224 , vit_base_patch16_224 = vit_base_patch16_224 , vit_base_patch16_384 = vit_base_patch16_384 ) module-attribute vit_base_patch16_224 () Source code in glasses/models/vision/classification/vit/zoo.py 16 17 18 19 def vit_base_patch16_224 (): return ViTForClassificationConfig ( backbone_config = ViTBackboneConfig ( depth = 12 , num_heads = 12 , forward_expansion = 4 , qkv_bias = True ), head_config = LinearHeadConfig ( 768 , 1000 ) ) vit_base_patch16_384 () Source code in glasses/models/vision/classification/vit/zoo.py 21 22 23 24 def vit_base_patch16_384 (): return ViTForClassificationConfig ( backbone_config = ViTBackboneConfig ( img_size = 384 , depth = 12 , num_heads = 12 , forward_expansion = 4 , qkv_bias = True ), head_config = LinearHeadConfig ( 768 , 1000 ) ) vit_small_patch16_224 () Source code in glasses/models/vision/classification/vit/zoo.py 10 11 12 13 def vit_small_patch16_224 (): return ViTForClassificationConfig ( backbone_config = ViTBackboneConfig ( depth = 8 , num_heads = 8 , forward_expansion = 3 ), head_config = LinearHeadConfig ( 768 , 1000 ) )","title":"zoo"},{"location":"reference/models/vision/classification/vit/zoo/#glasses.models.vision.classification.vit.zoo.zoo","text":"","title":"zoo"},{"location":"reference/models/vision/classification/vit/zoo/#glasses.models.vision.classification.vit.zoo.vit_base_patch16_224","text":"Source code in glasses/models/vision/classification/vit/zoo.py 16 17 18 19 def vit_base_patch16_224 (): return ViTForClassificationConfig ( backbone_config = ViTBackboneConfig ( depth = 12 , num_heads = 12 , forward_expansion = 4 , qkv_bias = True ), head_config = LinearHeadConfig ( 768 , 1000 ) )","title":"vit_base_patch16_224()"},{"location":"reference/models/vision/classification/vit/zoo/#glasses.models.vision.classification.vit.zoo.vit_base_patch16_384","text":"Source code in glasses/models/vision/classification/vit/zoo.py 21 22 23 24 def vit_base_patch16_384 (): return ViTForClassificationConfig ( backbone_config = ViTBackboneConfig ( img_size = 384 , depth = 12 , num_heads = 12 , forward_expansion = 4 , qkv_bias = True ), head_config = LinearHeadConfig ( 768 , 1000 ) )","title":"vit_base_patch16_384()"},{"location":"reference/models/vision/classification/vit/zoo/#glasses.models.vision.classification.vit.zoo.vit_small_patch16_224","text":"Source code in glasses/models/vision/classification/vit/zoo.py 10 11 12 13 def vit_small_patch16_224 (): return ViTForClassificationConfig ( backbone_config = ViTBackboneConfig ( depth = 8 , num_heads = 8 , forward_expansion = 3 ), head_config = LinearHeadConfig ( 768 , 1000 ) )","title":"vit_small_patch16_224()"},{"location":"reference/models/vision/necks/Neck/","text":"Neck Bases: nn . Module Source code in glasses/models/vision/necks/Neck.py 6 7 8 class Neck ( nn . Module ): def forward ( self , features : List [ Tensor ]) -> List [ Tensor ]: raise NotImplemented forward ( features ) Source code in glasses/models/vision/necks/Neck.py 7 8 def forward ( self , features : List [ Tensor ]) -> List [ Tensor ]: raise NotImplemented","title":"Neck"},{"location":"reference/models/vision/necks/Neck/#glasses.models.vision.models.Neck.Neck","text":"Bases: nn . Module Source code in glasses/models/vision/necks/Neck.py 6 7 8 class Neck ( nn . Module ): def forward ( self , features : List [ Tensor ]) -> List [ Tensor ]: raise NotImplemented","title":"Neck"},{"location":"reference/models/vision/necks/Neck/#glasses.models.vision.models.Neck.Neck.forward","text":"Source code in glasses/models/vision/necks/Neck.py 7 8 def forward ( self , features : List [ Tensor ]) -> List [ Tensor ]: raise NotImplemented","title":"forward()"},{"location":"reference/storage/","text":"","title":"Index"},{"location":"reference/storage/base/","text":"Storage Bases: ABC Source code in glasses/storage/base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Storage ( ABC ): @abstractmethod def put ( self , state_dict : StateDict , config : Dict ): pass @abstractmethod def get ( self , key : str ) -> Tuple [ StateDict , Dict ]: pass @property @abstractmethod def models ( self ) -> List [ str ]: pass def __contains__ ( self , key : str ) -> bool : return key in self . models __contains__ ( key ) Source code in glasses/storage/base.py 24 25 def __contains__ ( self , key : str ) -> bool : return key in self . models get ( key ) abstractmethod Source code in glasses/storage/base.py 15 16 17 @abstractmethod def get ( self , key : str ) -> Tuple [ StateDict , Dict ]: pass models () abstractmethod property Source code in glasses/storage/base.py 19 20 21 22 @property @abstractmethod def models ( self ) -> List [ str ]: pass put ( state_dict , config ) abstractmethod Source code in glasses/storage/base.py 11 12 13 @abstractmethod def put ( self , state_dict : StateDict , config : Dict ): pass","title":"base"},{"location":"reference/storage/base/#glasses.storage.base.Storage","text":"Bases: ABC Source code in glasses/storage/base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Storage ( ABC ): @abstractmethod def put ( self , state_dict : StateDict , config : Dict ): pass @abstractmethod def get ( self , key : str ) -> Tuple [ StateDict , Dict ]: pass @property @abstractmethod def models ( self ) -> List [ str ]: pass def __contains__ ( self , key : str ) -> bool : return key in self . models","title":"Storage"},{"location":"reference/storage/base/#glasses.storage.base.Storage.__contains__","text":"Source code in glasses/storage/base.py 24 25 def __contains__ ( self , key : str ) -> bool : return key in self . models","title":"__contains__()"},{"location":"reference/storage/base/#glasses.storage.base.Storage.get","text":"Source code in glasses/storage/base.py 15 16 17 @abstractmethod def get ( self , key : str ) -> Tuple [ StateDict , Dict ]: pass","title":"get()"},{"location":"reference/storage/base/#glasses.storage.base.Storage.models","text":"Source code in glasses/storage/base.py 19 20 21 22 @property @abstractmethod def models ( self ) -> List [ str ]: pass","title":"models()"},{"location":"reference/storage/base/#glasses.storage.base.Storage.put","text":"Source code in glasses/storage/base.py 11 12 13 @abstractmethod def put ( self , state_dict : StateDict , config : Dict ): pass","title":"put()"},{"location":"reference/storage/local/","text":"LocalStorage dataclass Bases: Storage Source code in glasses/storage/local/__init__.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @dataclass class LocalStorage ( Storage ): root : Path = Path ( \"/tmp/glasses\" ) override : bool = False fmt : str = \"pth\" def __post_init__ ( self ): self . root . mkdir ( exist_ok = True ) def put ( self , key : str , state_dict : StateDict , config : Dict ): save_dir = self . root / Path ( key ) save_dir . mkdir ( exist_ok = True ) model_save_path = save_dir / f \"model. { self . fmt } \" config_save_path = save_dir / f \"config.json\" if key not in self or self . override : torch . save ( state_dict , model_save_path ) with open ( config_save_path , \"w\" ) as f : json . dump ( config , f ) assert model_save_path . exists () assert config_save_path . exists () def get ( self , key : str ) -> Tuple [ StateDict , Config ]: save_dir = self . root / Path ( key ) model_save_path = save_dir / f \"model. { self . fmt } \" config_save_path = save_dir / f \"config.json\" state_dict = torch . load ( model_save_path ) with open ( config_save_path , \"r\" ) as f : config = json . load ( f ) return state_dict , config @property def models ( self ) -> List [ str ]: return [ file . stem for file in self . root . glob ( f \"*. { self . fmt } \" )] fmt : str = 'pth' class-attribute override : bool = False class-attribute root : Path = Path ( '/tmp/glasses' ) class-attribute __post_init__ () Source code in glasses/storage/local/__init__.py 21 22 def __post_init__ ( self ): self . root . mkdir ( exist_ok = True ) get ( key ) Source code in glasses/storage/local/__init__.py 37 38 39 40 41 42 43 44 def get ( self , key : str ) -> Tuple [ StateDict , Config ]: save_dir = self . root / Path ( key ) model_save_path = save_dir / f \"model. { self . fmt } \" config_save_path = save_dir / f \"config.json\" state_dict = torch . load ( model_save_path ) with open ( config_save_path , \"r\" ) as f : config = json . load ( f ) return state_dict , config models () property Source code in glasses/storage/local/__init__.py 46 47 48 @property def models ( self ) -> List [ str ]: return [ file . stem for file in self . root . glob ( f \"*. { self . fmt } \" )] put ( key , state_dict , config ) Source code in glasses/storage/local/__init__.py 24 25 26 27 28 29 30 31 32 33 34 35 def put ( self , key : str , state_dict : StateDict , config : Dict ): save_dir = self . root / Path ( key ) save_dir . mkdir ( exist_ok = True ) model_save_path = save_dir / f \"model. { self . fmt } \" config_save_path = save_dir / f \"config.json\" if key not in self or self . override : torch . save ( state_dict , model_save_path ) with open ( config_save_path , \"w\" ) as f : json . dump ( config , f ) assert model_save_path . exists () assert config_save_path . exists ()","title":"local"},{"location":"reference/storage/local/#glasses.storage.local.LocalStorage","text":"Bases: Storage Source code in glasses/storage/local/__init__.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @dataclass class LocalStorage ( Storage ): root : Path = Path ( \"/tmp/glasses\" ) override : bool = False fmt : str = \"pth\" def __post_init__ ( self ): self . root . mkdir ( exist_ok = True ) def put ( self , key : str , state_dict : StateDict , config : Dict ): save_dir = self . root / Path ( key ) save_dir . mkdir ( exist_ok = True ) model_save_path = save_dir / f \"model. { self . fmt } \" config_save_path = save_dir / f \"config.json\" if key not in self or self . override : torch . save ( state_dict , model_save_path ) with open ( config_save_path , \"w\" ) as f : json . dump ( config , f ) assert model_save_path . exists () assert config_save_path . exists () def get ( self , key : str ) -> Tuple [ StateDict , Config ]: save_dir = self . root / Path ( key ) model_save_path = save_dir / f \"model. { self . fmt } \" config_save_path = save_dir / f \"config.json\" state_dict = torch . load ( model_save_path ) with open ( config_save_path , \"r\" ) as f : config = json . load ( f ) return state_dict , config @property def models ( self ) -> List [ str ]: return [ file . stem for file in self . root . glob ( f \"*. { self . fmt } \" )]","title":"LocalStorage"},{"location":"reference/storage/local/#glasses.storage.local.LocalStorage.fmt","text":"","title":"fmt"},{"location":"reference/storage/local/#glasses.storage.local.LocalStorage.override","text":"","title":"override"},{"location":"reference/storage/local/#glasses.storage.local.LocalStorage.root","text":"","title":"root"},{"location":"reference/storage/local/#glasses.storage.local.LocalStorage.__post_init__","text":"Source code in glasses/storage/local/__init__.py 21 22 def __post_init__ ( self ): self . root . mkdir ( exist_ok = True )","title":"__post_init__()"},{"location":"reference/storage/local/#glasses.storage.local.LocalStorage.get","text":"Source code in glasses/storage/local/__init__.py 37 38 39 40 41 42 43 44 def get ( self , key : str ) -> Tuple [ StateDict , Config ]: save_dir = self . root / Path ( key ) model_save_path = save_dir / f \"model. { self . fmt } \" config_save_path = save_dir / f \"config.json\" state_dict = torch . load ( model_save_path ) with open ( config_save_path , \"r\" ) as f : config = json . load ( f ) return state_dict , config","title":"get()"},{"location":"reference/storage/local/#glasses.storage.local.LocalStorage.models","text":"Source code in glasses/storage/local/__init__.py 46 47 48 @property def models ( self ) -> List [ str ]: return [ file . stem for file in self . root . glob ( f \"*. { self . fmt } \" )]","title":"models()"},{"location":"reference/storage/local/#glasses.storage.local.LocalStorage.put","text":"Source code in glasses/storage/local/__init__.py 24 25 26 27 28 29 30 31 32 33 34 35 def put ( self , key : str , state_dict : StateDict , config : Dict ): save_dir = self . root / Path ( key ) save_dir . mkdir ( exist_ok = True ) model_save_path = save_dir / f \"model. { self . fmt } \" config_save_path = save_dir / f \"config.json\" if key not in self or self . override : torch . save ( state_dict , model_save_path ) with open ( config_save_path , \"w\" ) as f : json . dump ( config , f ) assert model_save_path . exists () assert config_save_path . exists ()","title":"put()"},{"location":"reference/tests/conftest/","text":"","title":"conftest"},{"location":"reference/tests/fixtures/","text":"TestAutoModel Bases: AutoModel Source code in glasses/tests/fixtures.py 28 29 class TestAutoModel ( AutoModel ): names_to_configs = { \"test1\" : TestConfig (), \"test2\" : TestConfig ( 2 )} names_to_configs = { 'test1' : TestConfig (), 'test2' : TestConfig ( 2 )} class-attribute TestConfig dataclass Bases: Config Source code in glasses/tests/fixtures.py 19 20 21 22 23 24 25 @dataclass class TestConfig ( Config ): in_channels : int = 1 out_channels : int = 2 def build ( self ): return TestModel ( ** self . __dict__ ) in_channels : int = 1 class-attribute out_channels : int = 2 class-attribute build () Source code in glasses/tests/fixtures.py 24 25 def build ( self ): return TestModel ( ** self . __dict__ ) TestModel Bases: nn . Module Source code in glasses/tests/fixtures.py 10 11 12 13 14 15 16 class TestModel ( nn . Module ): def __init__ ( self , in_channels , out_channels ): super () . __init__ () self . fc = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) def forward ( self , x ): return self . fc ( x ) fc = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) instance-attribute __init__ ( in_channels , out_channels ) Source code in glasses/tests/fixtures.py 11 12 13 def __init__ ( self , in_channels , out_channels ): super () . __init__ () self . fc = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) forward ( x ) Source code in glasses/tests/fixtures.py 15 16 def forward ( self , x ): return self . fc ( x ) test_auto_model () Source code in glasses/tests/fixtures.py 42 43 44 @pytest . fixture def test_auto_model (): return TestAutoModel test_config () Source code in glasses/tests/fixtures.py 32 33 34 @pytest . fixture def test_config (): return TestConfig () test_model_func () Source code in glasses/tests/fixtures.py 37 38 39 @pytest . fixture def test_model_func (): return TestModel","title":"fixtures"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestAutoModel","text":"Bases: AutoModel Source code in glasses/tests/fixtures.py 28 29 class TestAutoModel ( AutoModel ): names_to_configs = { \"test1\" : TestConfig (), \"test2\" : TestConfig ( 2 )}","title":"TestAutoModel"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestAutoModel.names_to_configs","text":"","title":"names_to_configs"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestConfig","text":"Bases: Config Source code in glasses/tests/fixtures.py 19 20 21 22 23 24 25 @dataclass class TestConfig ( Config ): in_channels : int = 1 out_channels : int = 2 def build ( self ): return TestModel ( ** self . __dict__ )","title":"TestConfig"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestConfig.in_channels","text":"","title":"in_channels"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestConfig.out_channels","text":"","title":"out_channels"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestConfig.build","text":"Source code in glasses/tests/fixtures.py 24 25 def build ( self ): return TestModel ( ** self . __dict__ )","title":"build()"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestModel","text":"Bases: nn . Module Source code in glasses/tests/fixtures.py 10 11 12 13 14 15 16 class TestModel ( nn . Module ): def __init__ ( self , in_channels , out_channels ): super () . __init__ () self . fc = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) def forward ( self , x ): return self . fc ( x )","title":"TestModel"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestModel.fc","text":"","title":"fc"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestModel.__init__","text":"Source code in glasses/tests/fixtures.py 11 12 13 def __init__ ( self , in_channels , out_channels ): super () . __init__ () self . fc = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 )","title":"__init__()"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.TestModel.forward","text":"Source code in glasses/tests/fixtures.py 15 16 def forward ( self , x ): return self . fc ( x )","title":"forward()"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.test_auto_model","text":"Source code in glasses/tests/fixtures.py 42 43 44 @pytest . fixture def test_auto_model (): return TestAutoModel","title":"test_auto_model()"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.test_config","text":"Source code in glasses/tests/fixtures.py 32 33 34 @pytest . fixture def test_config (): return TestConfig ()","title":"test_config()"},{"location":"reference/tests/fixtures/#glasses.tests.fixtures.test_model_func","text":"Source code in glasses/tests/fixtures.py 37 38 39 @pytest . fixture def test_model_func (): return TestModel","title":"test_model_func()"},{"location":"reference/tests/auto/test_auto/","text":"test_auto_model ( test_config , test_model_func , test_auto_model ) Source code in glasses/tests/auto/test_auto.py 4 5 6 7 8 9 10 11 12 13 def test_auto_model ( test_config , test_model_func , test_auto_model ): model = test_auto_model . from_name ( \"test1\" ) assert isinstance ( model , test_model_func ) config = test_auto_model . get_config_from_name ( \"test1\" ) assert isinstance ( config , type ( test_config )) assert config == test_config","title":"test_auto"},{"location":"reference/tests/auto/test_auto/#glasses.tests.tests.test_auto.test_auto_model","text":"Source code in glasses/tests/auto/test_auto.py 4 5 6 7 8 9 10 11 12 13 def test_auto_model ( test_config , test_model_func , test_auto_model ): model = test_auto_model . from_name ( \"test1\" ) assert isinstance ( model , test_model_func ) config = test_auto_model . get_config_from_name ( \"test1\" ) assert isinstance ( config , type ( test_config )) assert config == test_config","title":"test_auto_model()"},{"location":"reference/tests/auto/test_auto_utils/","text":"test_get_names_to_configs_map () Source code in glasses/tests/auto/test_auto_utils.py 4 5 6 7 8 def test_get_names_to_configs_map (): zoo = get_names_to_configs_map ( \"package\" ) assert \"a1\" in zoo assert \"a2\" in zoo assert \"b1\" in zoo","title":"test_auto_utils"},{"location":"reference/tests/auto/test_auto_utils/#glasses.tests.tests.test_auto_utils.test_get_names_to_configs_map","text":"Source code in glasses/tests/auto/test_auto_utils.py 4 5 6 7 8 def test_get_names_to_configs_map (): zoo = get_names_to_configs_map ( \"package\" ) assert \"a1\" in zoo assert \"a2\" in zoo assert \"b1\" in zoo","title":"test_get_names_to_configs_map()"},{"location":"reference/tests/auto/package/","text":"","title":"Index"},{"location":"reference/tests/auto/package/a/","text":"","title":"Index"},{"location":"reference/tests/auto/package/a/model/","text":"","title":"model"},{"location":"reference/tests/auto/package/a/zoo/","text":"zoo = dict ( a1 = 1 , a2 = 2 ) module-attribute","title":"zoo"},{"location":"reference/tests/auto/package/a/zoo/#glasses.tests.tests.package.a.zoo.zoo","text":"","title":"zoo"},{"location":"reference/tests/auto/package/b/","text":"","title":"Index"},{"location":"reference/tests/auto/package/b/model/","text":"","title":"model"},{"location":"reference/tests/auto/package/b/zoo/","text":"zoo = dict ( b1 = 1 ) module-attribute","title":"zoo"},{"location":"reference/tests/auto/package/b/zoo/#glasses.tests.tests.package.b.zoo.zoo","text":"","title":"zoo"},{"location":"tutorial/configurations/","text":"Glasses uses a configuration system to record/share and load custom version of a specific architecture. The main idea behind our configuration system is to be an addition to the models, not a requirement . Any model in classes can be created by just importing it and passing the right parameters, they don't know about configurations. Saying that, why do we need configurations? Configurations are necessary when we need to store a specific set of parameters for a model. For example, if a model was trained on dataset X with ten classes, our configuration will contain all the parameters need to create that specific model. In most libraries, configuration are serialized files (e.g. yml ), in glasses they are piece of code. This allow the user to take advante of it's IDE and see the parameters at any point in time. Let's see how to create a basic config. First, we need a model from torch import nn class MyModel ( nn . Module ): def __init__ ( in_channels : int , out_channels : int ): super () . __init__ () self . conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 ) def forward ( self , x ): return self . conv ( x ) Then we can create it's configuration, from glasses.config import Config # Let's create it's configuration @dataclass class MyConfig ( Config ): in_channels : int out_channels : int def build ( self ) -> nn . Module : # create a `MyModel` instance using `MyConfig` return MyModel ( ** self . __dict__ ) We can now invoke the build method, that will create the model model : MyModel = MyConfig ( 2 , 2 ) . build () # same as model : MyModel = MyModel ( 2 , 2 ) Nothing very special. Let's see how to create a nested config . Assume we have a model takes a backbone and idk has a fixed head. from torch import nn class MyModel ( nn . Module ): def __init__ ( self , backbone : nn . Module , channels : int , num_classes : int ): super () . __init__ () self . backbone = backbone self . head = nn . Conv2d ( channels , num_classes , kernel_size = 1 ) def forward ( self , x ): features = self . backbone ( x ) out = self . head ( features [ - 1 ]) # use last feature return out Our config will be nested from glasses.config import Config @dataclass class MyConfig ( Config ): backbone_config : Config channels : int num_classes : int def build ( self ) -> nn . Module : backbone = backbone_config . build () # create a `MyModel` instance using `MyConfig` return MyModel ( backbone , self . channels , self . num_classes ) Obliously, we must have configs for the different backbones we want to use. from torch import nn from glasses.config import Config class BackboneA ( nn . Module ): def __init__ ( ... ): ... @dataclass class BackboneAConfig ( Config ): ... class BackboneB ( nn . Module ): def __init__ ( ... ): .... @dataclass class BackboneBConfig ( Config ): ... Then, we can pass any backbone to MyConfig . config = MyConfig ( backbone_config = BackboneAConfig ( ... ), ... ) config . build () # build model with backbone A config = MyConfig ( backbone_config = BackboneBConfig ( ... ), ... ) config . build () # build model with backbone B The main advantage of the config system is when we need to save specific model version. For instance, assume I have trained MyModel with BackboneA on dataset X . It's config will look like: my_model_backbone_a_x = MyConfig ( backbone_config = BackboneAConfig ( ... ), channels = 64 , num_classes = 10 ) Therefore, at any point in time I can recreate the model and load it's pretrained weights. my_model_backbone_a_x . build () . load_state_dict ( \"/somewhere/my_model_backbone_a_x.pth\" ) Now, what if I want to use my_model_backbone_a_x architecture but just change a small part? Maybe the numer of classes? # clone the config config = MyConfig ( ** my_model_backbone_a_x . __dict__ ) config . num_classes = 8 # load the pretrained weight, with a different number of classes config . build () . load_state_dict ( \"/somewhere/my_model_backbone_a_x.pth\" )","title":"Configurations"}]}